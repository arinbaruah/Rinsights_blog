[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arindam (Aaron) Baruah",
    "section": "",
    "text": "Arindam (Aaron) Baruah\nI am a current final year graduate student of Business Analytics at Monash Business School, Monash University. üßë‚Äçüéì\nWith a strong foundation from my academic journey, I bring two years of hands-on experience as a data analyst at ArcelorMittal and Nippon Steel, where I honed my analytical skills in a real-world setting. Additionally, I‚Äôve spent two years as a teaching assistant at The Indian Institute of Technology Indore, contributing to research and development in the field of metallurgy. ‚öôÔ∏èüïµÔ∏è‚Äç‚ôÇÔ∏èüñ•Ô∏è\nI‚Äôm passionate about leveraging data-driven insights and analytical expertise to solve complex business as well as research challenges. üìä\nPlease feel free to go through the links below to learn more about my experiences, skills, current projects and ways to get in touch with me. üì§  Important links \n\n\n\n\n\n\nInterested in analytics content ?\n\n\n\nFor those interested in uncovering data and what is expected when studying a graduate course in the field of Business Analytics, I post informative content on my blog, R-insights. Be sure to check it out !\n\n\n\n\nGithub Activity"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi ! I am Arindam Baruah, a graduate student of Business Analytics at Monash Business School, Monash University. With a keen interest and salient experiences in the field of analytics, I am intrigued by the patterns which can be uncovered through data visualisations and some very interesting mathematics. I focus on implementing data science concepts to real-world problems and have successfully implemented them for predicting various estimates in the field of manufacturing.\n\n\n\n\n\n\nMajor interests\n\n\n\nStatistics, Data analytics in the manufacturing industry and formula 1, Machine learning, Steel manufacturing.\n\n\nMy studies generally span over various fields and I am constantly in the hunt to uncover more scenarios which would let me harness the power of analytics to automate various tasks and allow me to gain meaningful insights post analysis.\n\nMy journey so far‚Ä¶\n\nArcelorMittal Nippon steelIndian Institute of Technology\n\n\n\n\n\n\n\n\nSkills\n\n\n\nData Analysis, MS Office, Data Visualisation, Python, Customer Relations, Team Work, MS PowerBI, Communication Skills, Root Cause Analysis, Conflict Management.\n\n\n\n\nTechnical Data Analyst (2021-23)\nAfter studying a specialisation in the field of metallurgy and manufacturing engineering at IIT Indore, I joined a leading steel manufacturer in the world, ArcelorMittal in their joint venture with Nippon Steel, AM/NS India.\nAs part of my deployment, I was posted as a technical data analyst in the compact strip production (CSP) division of the integrated steel plant in the region of Hazira, Gujarat, India. Here are some key highlights of my wonderful time back in the steel industry:\n\nPerformed data-driven time and motion-based study to reduce downtime of the production unit resulting in potential annual cost savings of 1.8 million dollar.\nCooperated with multiple departments and team leaders to provide sample data analysis and draw conclusions through intuitive visualisations.\nCreated a process monitoring dashboard on PowerBI for quick anomaly detection, which allowed for regular corrective and preventive actions to be taken on short notice and further reduced data cleaning operation duration from 1 hour to 20 minutes.\nCompiled, maintained and visualised data containing critical sustainability indicators for studying the effect of the manufacturing plant on the environment.\nWorked closely with R&D, Quality control and Automation department for resolving process and product defects through data driven studies.\nPerformed root cause analysis and suggested corrective actions for major product defects.\nConducted technical and software-based trainings for shop floor operators.\nImplemented various statistical quality control techniques, such as control charts, to study and monitor the current process deviations before and after any significant process alteration through sample analysis.\nInvolved in investigation of customer or downstream complaints and suggesting corrective actions to prevent recurrence of similar complaints.\n\n\n\n\n\n\n\n\n\n\nSkills\n\n\n\nData Analysis, MS Office, Data Visualisation, Python, Research, Metallurgy, Computer Aided Drawing, Communication Skills, Root Cause Analysis, Finite Element Analysis.\n\n\n\n\nTeaching Assistant (2019-21)\nAs a teaching assistant at the Indian Institute of Technology Indore, I was a part of the Advanced Mechanical Metallurgy laboratory, mostly working with various computational methods such as machine learning classification models and finite element models to detect defects in solid state welding processes such as Friction Stir Welding.\nSome of my key highlights while in academia are as follows:\n\nConducte detailed experimentation and perform data analysis of the results from various metallurgical studies such as X-ray diffraction, electron microscopy and universal testing machines.\nUtilised machine learning models such as XGBoost, and LightGBM to train datasets which would allow for accurate detection of process parameters resulting in joint defects such as voids in welded mechanical joints.\nNominated as a delegate to showcase my research at the Annual Technical Meet hosted by The Indian Institute of Metals and was awarded with the best poster presentation for my research on creating a finite element model of aluminium and magnesium welded friction stir welded joints.\nServed as a key member of the placement committee which was tasked with maintaing industrial relations with various organisations to conduct recruitment programs for the current cohort of students.\nServed as a class representative who was responsible for representing the class of 2021 in various redressal meetings with the faculties of the institute.\nSuccessfully published a total of 3 research papers during my tenure at the institute which can be referred to here.\n\n\n\n\n\n\n\nRecent publications\n\nOptimised machine learning classification model to detect void formations in friction stir welding. Link\nNumerically modelled study of the plunge stage in friction stir spot welding using multi-tiered mesh partitions. Link\nNumerical Simulation of Friction Stir Spot Welding of Aluminium-6061 and Magnesium AZ-31B. Link"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "R-insights : Informative and insightful analytical blogs",
    "section": "",
    "text": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶\n\n\n\n\n\n\nanalysis\n\n\nexploratory data analysis\n\n\nvisualisation\n\n\nstatistics\n\n\nmachine learning\n\n\nclassification model\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nArindam Baruah\n\n\n\n\n\n\n  \n\n\n\n\nEquity in Academia: A New Era of Gender Diversity in US PhD Programs üßë‚Äçüéìüë©‚Äçüéì\n\n\n\n\n\n\nanalysis\n\n\nexploratory data analysis\n\n\nvisualisation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nArindam Baruah\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the Mosaic: A Deep Dive into the Educational Diversity Amongst US PhD Graduates üßë‚Äçüéìüìöüá∫üá∏\n\n\n\n\n\n\nanalysis\n\n\nvisualisation\n\n\nexploratory data analysis\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nArindam Baruah\n\n\n\n\n\n\n  \n\n\n\n\n‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?\n\n\n\n\n\n\nanalysis\n\n\nexploratory data analysis\n\n\nvisualisation\n\n\nstatistics\n\n\nmachine learning\n\n\nregression\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nArindam Baruah\n\n\n\n\n\n\n  \n\n\n\n\nAll aboard the blog train !\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nArindam Baruah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blogs/welcome/index.html",
    "href": "posts/blogs/welcome/index.html",
    "title": "All aboard the blog train !",
    "section": "",
    "text": "Hey you curious heads !\nThankyou for choosing to spend some time on my blogsite. My work showcased on this blog is based on analysing and visualising thousands of rows of data, with the aim to decompress and make the study interesting as well as meaningful. A large part of this is possible as a result of the open-source software and packages of R programming. Please feel free to explore the various blogs which I have penned down (not literally üòÖ).\n\n\n\n\n THE MIND THAT OPENS TO A NEW IDEA NEVER RETURNS TO ITS ORIGINAL SIZE !\n\n\n - Albert Einstein"
  },
  {
    "objectID": "posts/blogs/post-with-code/index.html",
    "href": "posts/blogs/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html#arcelormittal-nippon-steel",
    "href": "about.html#arcelormittal-nippon-steel",
    "title": "About",
    "section": "ArcelorMittal Nippon steel",
    "text": "ArcelorMittal Nippon steel\nAfter studying a specialisation in the field of metallurgy and manufacturing engineering at IIT Indore, I joined the world‚Äôs second largest steel manufacturer in the world, ArcelorMittal in their joint venture with Nippon Steel, AM/NS India. As part of my deployment, I was posted in the compact strip production (CSP) division of the integrated steel plant in the region of Hazira, Gujarat, India."
  },
  {
    "objectID": "posts/blogs/gender_diversity/index.html#sec-shift",
    "href": "posts/blogs/gender_diversity/index.html#sec-shift",
    "title": "Equity in Academia: A New Era of Gender Diversity in US PhD Programs üßë‚Äçüéìüë©‚Äçüéì",
    "section": "5.1 Are we observing a shift towards more women in research ?",
    "text": "5.1 Are we observing a shift towards more women in research ?\nIn order to answer this question, we need to look into the gender diversity among the PhD graduates in the previous century. Consequently, we will attempt to study the trend of the PhD graduates for the years prior to the year of 2000 and compare the current gender diversity in years after 2000.\n\n\n\n\n\nFigure¬†1: Change in gender diversity among PhD graduates in the US before and after 2000\n\n\n\n\nFigure¬†1 illustrates the number of male and female PhD graduates before and after the year 2000. We can observe that the number of male PhD graduates have remained fairly constant during the period between 1949-2000 while the number of female PhD graduates started from a very small group in 1950 but have been constantly on a rise till the end of the century. This suggests that the research field in the United States have witnessed an encouraging rise in the gender distribution among the PhD graduates.\nIn the current century, we have observed that the number of female PhD graduates are constantly on a rise and have even gone on to exceed the number of male PhD graduates in the United States.\n\n\n\n\n\n\nKey takeaway\n\n\n\nEven though women have had to face discrimination in the field of research, however, their story of persevering through the hardships and in large numbers as observed through their meteoric rise in the PhD graduates indicates that there is a flourishing and encouraging gender diversity among the PhD graduates in the United States today."
  },
  {
    "objectID": "posts/blogs/gender_diversity/index.html#when-did-the-major-shift-happen",
    "href": "posts/blogs/gender_diversity/index.html#when-did-the-major-shift-happen",
    "title": "Equity in Academia: A New Era of Gender Diversity in US PhD Programs üßë‚Äçüéìüë©‚Äçüéì",
    "section": "5.2 When did the major shift happen üî¨?",
    "text": "5.2 When did the major shift happen üî¨?\nAs explained and visualised in Section¬†5.1, the number of women PhD graduates were on a constant rise ever since the year of 1950 and have now exceeded the number of male PhD graduates. We would like to observe the exact time period when this major milestone occured.\n\n\n\n\n\nFigure¬†2: Difference between number of female and male PhD graduates in The United States\n\n\n\n\nFigure¬†2 illustrates the difference between the number of female and male PhD graduates in The United States. As concluded in previous sections, we have established a constant rise in the number of female PhD graduates. However, only in the academic year of 2005-06, we have observed that the number of female PhD graduates have now exceeded the number of male PhD graduates in The US.\n\n\n\n\n\n\nKey takeaway\n\n\n\nThe year of 2005-06 indeed marks a major milestone in the higher education history of The United States where female PhD graduates have now outnumbered male PhD graduates, indicating a flourishing rise of gender diversity in the country. The number of female PhD graduates have continued to grow at a constant pace and have been observed to exceed the male graduates by nearly 20000 in the academic year of 2019-20."
  },
  {
    "objectID": "posts/blogs/gender_diversity/index.html#can-we-use-a-better-statistic-to-understand-this-data",
    "href": "posts/blogs/gender_diversity/index.html#can-we-use-a-better-statistic-to-understand-this-data",
    "title": "Equity in Academia: A New Era of Gender Diversity in US PhD Programs üßë‚Äçüéìüë©‚Äçüéì",
    "section": "5.3 Can we use a better statistic to understand this data ü§î?",
    "text": "5.3 Can we use a better statistic to understand this data ü§î?\nWhile we have dealt with the raw number of PhD graduates for each gender so far, however, to make our understanding more intuitive, we rely upon the metric of gender ratio. Such a metric is often used as a key indicator to study the gender distribution. In our analysis, the gender ratio will be defined as the ratio of the number of female PhD graduates to the male PhD graduates.\n\n\n\n\n\nFigure¬†3: Change in gender ratio among US PhD graduates\n\n\n\n\nFigure¬†3 provides a visual illustration of the constant rise in number of female PhD graduates in The United States. In the year of 1949-50, the gender ratio was observed to be a measly 0.11 which has grown to 1.23 by the year of 2019-20.\n\n\n\n\n\n\nKey takeaway\n\n\n\nUsage of the gender ratio metric allows us to investigate the gender diversity and the gender gap without having to look into the detailed numbers and calculations. This makes the analysis much more intuitive and easy to understand."
  },
  {
    "objectID": "posts/blogs/field_diversity/index.html#sec-sec1",
    "href": "posts/blogs/field_diversity/index.html#sec-sec1",
    "title": "Unveiling the Mosaic: A Deep Dive into the Educational Diversity Amongst US PhD Graduates üßë‚Äçüéìüìöüá∫üá∏",
    "section": "5.1 Are we observing a rise in the popularity of research ?",
    "text": "5.1 Are we observing a rise in the popularity of research ?\nWhile the universities and research labs across the United States have historically produced some of the best researchers in the world, it is important to study the growth of these researchers. One of the ways to do so would be through analysing the number of graduating PhDs in the various fields of research.\n\n\n\n\n\nFigure¬†1: Number of graduating PhDs in the US\n\n\n\n\nAs illustrated through Figure¬†1, we can observe a trend of rising PhD graduates in the United States. Between the years of 2008 to 2017, apart from the year of 2010, there has been a constant rise in the number of PhD students graduating across the various universities and research labs in the country. Since 2008 to 2017, we can observe a 12.6 % rise in the total number of graduating PhD students as per the statistics released by The National Science Foundation (NSF). The rise in these numbers can be attributed to the funding received in various fields of research, leading to higher recruitment and consequently, higher number of PhD graduates. This additionally makes the United States a very attractive study destination for the rest of the world. As a result, we can definitely observe a rise in the popularity of the research field in The United States of America."
  },
  {
    "objectID": "posts/blogs/field_diversity/index.html#which-fields-of-research-are-these-phds-graduating-in",
    "href": "posts/blogs/field_diversity/index.html#which-fields-of-research-are-these-phds-graduating-in",
    "title": "Unveiling the Mosaic: A Deep Dive into the Educational Diversity Amongst US PhD Graduates üßë‚Äçüéìüìöüá∫üá∏",
    "section": "5.2 Which fields of research are these PhDs graduating in ?",
    "text": "5.2 Which fields of research are these PhDs graduating in ?\nIn this section, we shall investigate the popular areas of research. While there has been an overall rise in the number of graduating PhDs as explained in Section¬†5.1, however, not all broad fields of research may observe the same growth in popularity. Figure¬†2 illustrates the distribution of the total number of PhD graduates in each of the broad fields of research. We can observe that the field of Life Sciences attracts the highest number of PhDs while the least number of PhD graduates can be observed in the STEM field of Engineering.\nThe results corroborate with the findings of Meirmans et al. (2019) who have identified the field of biological life sciences to be the one with the highest fundings worldwide as these researches are directly related to various answering some of the toughest challenges to mankind such as curing cancer, eradication of polio, epidemic research as such COVID-19, and many such critical studies.\nOn the other hand, the field of engineering is observed to attract the least number of researchers. A part of the reason maybe due to the vast number of opportunities in the industry or the corporate which tends to attract more number of engineering graduates rather than the field of research.\n\n\n\n\n\nFigure¬†2: PhD graduates in each broad field"
  },
  {
    "objectID": "posts/blogs/field_diversity/index.html#sec-sec2",
    "href": "posts/blogs/field_diversity/index.html#sec-sec2",
    "title": "Unveiling the Mosaic: A Deep Dive into the Educational Diversity Amongst US PhD Graduates üßë‚Äçüéìüìöüá∫üá∏",
    "section": "5.3 A deeper look into the field of Life Sciences üî¨ !",
    "text": "5.3 A deeper look into the field of Life Sciences üî¨ !\nNow that we have established the broad field of Life Sciences to be the one with the most number of PhD graduates,let us look deeper into this field and whether we can break down the distribution further.\n\n\n\n\n\nFigure¬†3: Distribution of PhD graduates within the field of Life Science\n\n\n\n\nFigure¬†3 illustrates distribution of the PhD graduates within the broad field of Life Sciences. As we can observe, there is indeed an uneven distribution of graduates in the various major fields of Life Sciences. We can observe that the number of graduates in the domain of biological and biomedical sciences is more than double the number of students in the second most popular field which is the field of Physics and astronomy.\n\nThis clearly suggests that the requirement for researchers and the funding available in the major field of biological and biomedical sciences is considerable higher than any other major field of Life Sciences."
  },
  {
    "objectID": "posts/blogs/field_diversity/index.html#how-is-the-distribution-amongst-the-engineering-graduates",
    "href": "posts/blogs/field_diversity/index.html#how-is-the-distribution-amongst-the-engineering-graduates",
    "title": "Unveiling the Mosaic: A Deep Dive into the Educational Diversity Amongst US PhD Graduates üßë‚Äçüéìüìöüá∫üá∏",
    "section": "5.4 How is the distribution amongst the engineering graduates ‚úàÔ∏è ‚öôÔ∏è?",
    "text": "5.4 How is the distribution amongst the engineering graduates ‚úàÔ∏è ‚öôÔ∏è?\nNow that we have looked into the most popular field of research in Section¬†5.3, let us now investigate the distribution of PhD graduates in the field with the least number of total PhD graduates, which is the field of engineering.\n\n\n\n\n\nFigure¬†4: Distribution of PhD graduates within the field of Engineering\n\n\n\n\nFigure¬†4 illustrates a far larger number of fields within the field of Engineering when compared to Life Sciences. However, similar to the distribution of Life Sciences, the PhD graduates in the field of Engineering are unevenly distributed with a clear trend in popularity for specific fields. As we can observe, the number of researchers in the field of Computer Engineering is much higher than the next popular field of Environmental Engineering.\n\nThis can be attributed to the recent advancements in the field of Artificial Intelligence (AI), Machine Learning (ML), Data Science, and many more state of the art technologies"
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html",
    "href": "posts/blogs/Bank_churn/index.html",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "",
    "text": "Source: Medium"
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#estimated-salary",
    "href": "posts/blogs/Bank_churn/index.html#estimated-salary",
    "title": "Bank account churn classification",
    "section": "5.1 Estimated Salary",
    "text": "5.1 Estimated Salary\nThe estimated salary variable indicates the income of each customer declared as a salary. Salaries can never be negative. Let us quickly check if that indeed is the case.\n\n\n\n\n\nFigure¬†2: Salary distribution of customers\n\n\n\n\nBased on the histrogram of the salaries as illustrated by Figure¬†2, we can observe that the salaries are indeed positive which is what is expected."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#age",
    "href": "posts/blogs/Bank_churn/index.html#age",
    "title": "Bank account churn classification",
    "section": "5.2 Age",
    "text": "5.2 Age\nBank account customers are generally required to be adults (&gt; 18 years). We will check if that holds true for the current dataset and attempt to detect any anomalous data such as negative age.\n\n\n\n\n\nFigure¬†3: Age distribution of the bank customers\n\n\n\n\nBased on Figure¬†3, we can observe that the data indeed suggests that the customers are of the right age (&gt;18 years) and there are no anomalous data entries for this variable.\n:::"
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#correlation-plot",
    "href": "posts/blogs/Bank_churn/index.html#correlation-plot",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.1 Correlation plot",
    "text": "6.1 Correlation plot\n\n\n\n\n\nFigure¬†4: Correlation plot\n\n\n\n\n\n\n\n\n\n\nKey takeaway\n\n\n\nBased on our understanding of the variables as illustrated by the correlation plot in Figure¬†4, we can infer that there is no single variable which is highly correlated to the churn indicator. We can also observe that none of the features are highly correlated to one another. This indicates that there is no multicollinearity in the choice of our features."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#geography-wise-churn",
    "href": "posts/blogs/Bank_churn/index.html#geography-wise-churn",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.2 Geography wise churn",
    "text": "6.2 Geography wise churn\nLet us try to understand if there are any geographical regions which have accounted for high churns.\n\n\n\n\n\nFigure¬†5: Bank accounts for each geographical location\n\n\n\n\nBased on the illustration Figure¬†5, we can observe that the percentage of churns are significantly higher in Germany as compared to France and Spain where the churn percentage appears to be similar.\n\n\n\n\n\n\nKey takeaway\n\n\n\nThe higher churn % for Germany could indicate issues pertaining to the economy of the country or factors such as loan interest rate in the country. It could also indicate that the banking services or customer services in Germany might be lacking as compared to the other countries which have resulted in the higher churn percentage. Diving deeper into the banking practices of Germany would provide a better understanding as to the reason for the high attrition of customers."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#age-wise-bank-churn",
    "href": "posts/blogs/Bank_churn/index.html#age-wise-bank-churn",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.3 Age wise bank churn",
    "text": "6.3 Age wise bank churn\n\n\n\n\n\nFigure¬†6: Age wise churn data\n\n\n\n\nAs we can observe through Figure¬†6, the churns are associated majorly with customers between the ages of 25 and 60.\n\n\n\n\n\n\nKey takeaway\n\n\n\nThe data indicates that after a couple of years since opening a new bank account, customers are observed to close their accounts which could be due to numerous reasons such as:\n\nBetter customer services from other banks\nLow credit score or due to financial bankruptcy of the customer\nHigh loan interests charged by the bank which may have led the customer to switch to another bank after fulfilling the loan."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#credit-score-wise-churn",
    "href": "posts/blogs/Bank_churn/index.html#credit-score-wise-churn",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.4 Credit score wise churn",
    "text": "6.4 Credit score wise churn\nLet us now try to analyse if the credit scores could tell us anything about the likelihood of the bank account churns.\n\n\n\n\n\nFigure¬†7: Credit score distribution of churned accounts\n\n\n\n\n\n\n\n\n\n\nKey takeaway\n\n\n\nBased on the credit score distribution of the churned and active accounts as illustrated in Figure¬†7, we cannot observe any discernible difference in the credit scores which could indicate the likeliness of a bank account churn. This indicates that the credit scores of customers may not be a strong factor leading to them closing their bank accounts."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#transaction-activity-wise-churn",
    "href": "posts/blogs/Bank_churn/index.html#transaction-activity-wise-churn",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.5 Transaction activity wise churn",
    "text": "6.5 Transaction activity wise churn\nNext, we can analyse if the churns are mostly from bank accounts which are inactive, possibly due to very low transactions.\n\n\n\n\n\nFigure¬†8: Transaction activity wise churn\n\n\n\n\n\n\n\n\n\n\nKey takeaway\n\n\n\nBased on Figure¬†8, we can observe that the percentage of the bank churns for inactive accounts are more than twice that of the active accounts.\nThis indicates that the probability of a churn rises if the transaction activity of the bank account reduces. This could also be measure taken by the bank to reduce the burden of keeping the services activated for accounts which have shown little to no activity for an extended period of duraiton."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#balance-wise-churn",
    "href": "posts/blogs/Bank_churn/index.html#balance-wise-churn",
    "title": "Detecting a possible event of bank churn using machine learning classification models üí∏üè¶",
    "section": "6.6 Balance wise churn",
    "text": "6.6 Balance wise churn\nLet us try to analyse if the account balance can indicate whether a churn might take place.\nFor this, we will flag any balance which is 0.\n\n\n\n\n\nFigure¬†9: Account balance wise churn\n\n\n\n\n\n\n\n\n\n\nKey takeaway\n\n\n\nFigure¬†9 illustrates the percentage of bank accounts churned for accounts with positive balance and zero balance. Contrary to belief, the percentage of churns are actually higher for accounts with positive balance as compared to zero balance accounts.\nThis could be due to the fact that account holders with zero balances are generally inactive and may not proactively close their accounts but rather, is done so by the bank to reduce the maintenance cost of keeping zero balance accounts active. However, account holders with positive balance may proactively choose to close their accounts due to various reasons as stated in Section¬†1 which may lead to the higher percentage of churned accounts."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#xgboost-classifier",
    "href": "posts/blogs/Bank_churn/index.html#xgboost-classifier",
    "title": "Bank account churn classification",
    "section": "7.1 XGboost classifier",
    "text": "7.1 XGboost classifier\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\n\n[1] train-logloss:0.430351 \n[2] train-logloss:0.381727 \n\n\n\n\n\n\n\nFigure¬†12: Confusion matrix for XGBoost classification model\n\n\n\n\nWe observe a marginally better classification accuracy as compared to the logistic regression model for the XGboosted classifier as depicted by Figure¬†12 with an accuracy of 85.1%."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#light-gbm",
    "href": "posts/blogs/Bank_churn/index.html#light-gbm",
    "title": "Bank account churn classification",
    "section": "7.2 Light GBM",
    "text": "7.2 Light GBM\nLet us utilise the LGBM algorithm and train it on the given dataset.\n\n\n\n\n\nFigure¬†13: Confusion matrix for Light GBM classification model\n\n\n\n\nBased on Figure¬†13, we can observe that the LGB classifier has actually classified poorly with multiple negatives observed in its classification. We can disregard this model for our purpose of classification."
  },
  {
    "objectID": "posts/blogs/Bank_churn/index.html#catboost-classifier",
    "href": "posts/blogs/Bank_churn/index.html#catboost-classifier",
    "title": "Bank account churn classification",
    "section": "7.3 Catboost classifier",
    "text": "7.3 Catboost classifier\n\n\n\n\n\nFigure¬†14: Importance of each variable based on the Catboost classification algorithm\n\n\n\n\n\n\n\n\n\nFigure¬†15: Confusion matrix for Catboost classification model\n\n\n\n\nUpon utilising the Catboost classifier, we can observe from Figure¬†15 that the classifier has a marginally better accuracy when compared to the logistic regression, XGboost and Light GBM.\nAdditionally, Figure¬†14 indicates the variables which have the highest importance in predicting the bank account churns. We can observe that the variables, ‚ÄúNumber of products‚Äù followed by ‚ÄúAge‚Äù and ‚ÄúActivity‚Äù are the most critical to be able to predict tge churned accounts.\n:::"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "",
    "text": "Binary machine failure prediction using machine learning is a technique employed to anticipate the occurrence of failures or malfunctions in a binary system or machine. With the increasing complexity of modern machines, the ability to predict and prevent failures becomes crucial for optimizing performance, reducing downtime, and avoiding costly repairs.\nMachine learning algorithms play a vital role in this prediction process by analyzing historical data and identifying patterns or anomalies that indicate potential failures. These algorithms learn from past failure instances, considering various factors such as sensor readings, environmental conditions, maintenance records, and other relevant parameters.\nThe predictive models are trained on labeled datasets, where each instance is associated with a failure or non-failure outcome. Common machine learning techniques used for binary machine failure prediction include logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks.\nDuring the training phase, the algorithms learn the relationships between input features and failure occurrences, thereby enabling them to make accurate predictions on unseen data. Feature engineering, which involves selecting or transforming relevant input variables, is an essential step in improving the model‚Äôs performance.\nOnce the model is trained, it can be deployed to make real-time predictions on new data streams. By continuously monitoring machine inputs and comparing them to the learned patterns, the system can generate alerts or take preventive actions whenever a potential failure is detected. This proactive approach helps minimize unexpected downtime, reduce maintenance costs, and improve overall operational efficiency.\nBinary machine failure prediction using machine learning is widely applied across various industries, including manufacturing, power generation, healthcare, transportation, and more. By leveraging the power of data and advanced analytics, it offers a valuable tool for optimizing maintenance strategies, enhancing productivity, and ensuring the reliability of critical systems.\n\nknitr::include_graphics(\"mach_failure.jpeg\")\n\n\n\n\nSource: www.gesrepair.com"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#check-for-null-values",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#check-for-null-values",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Check for null values",
    "text": "Check for null values\nAs a part of checking for the cleanliness of the dataset, let us visaulise the presence of null values for each of the variables.\n\ngg_miss_var(df_train)\n\n\n\n\nMissingness in the dataset\n\n\n\n\nAs we can observe from figure @ref(fig:missvis), there are no missing values for any of the variables in the dataset. As a result, the dataset can be considered clean for further analysis."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#removal-of-variables",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#removal-of-variables",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Removal of variables",
    "text": "Removal of variables\nAfter studying for the presence of null values, we now remove the variables that do not provide any extra insights into our analysis.\n\ndf_train &lt;- df_train %&gt;% select(-c(id,`Product ID`))"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#cleaning-the-variable-names",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#cleaning-the-variable-names",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Cleaning the variable names",
    "text": "Cleaning the variable names\nThe current dataset contains variable names which are not ideal for data wrangling and EDA. Hence, we will try to remove any unnecessary white space and special characters for each of the variable names.\n\ndf_train &lt;- clean_names(df_train)\nhead(df_train)\n\n# A tibble: 6 √ó 12\n  type  air_temperature_k process_temperature_k rotational_speed_rpm torque_nm\n  &lt;chr&gt;             &lt;dbl&gt;                 &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 L                  301.                  310.                 1596      36.1\n2 M                  303.                  312.                 1759      29.1\n3 L                  299.                  308.                 1805      26.5\n4 L                  301                   311.                 1524      44.3\n5 M                  298                   309                  1641      35.4\n6 M                  298.                  309.                 1429      42.1\n# ‚Ñπ 7 more variables: tool_wear_min &lt;dbl&gt;, machine_failure &lt;dbl&gt;, twf &lt;dbl&gt;,\n#   hdf &lt;dbl&gt;, pwf &lt;dbl&gt;, osf &lt;dbl&gt;, rnf &lt;dbl&gt;"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#type-of-machine",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#type-of-machine",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Type of machine",
    "text": "Type of machine\nThere are a total of 3 types machines in this dataset. These are encoded as:\n\nL (Light)\nM (Medium)\nH (Heavy)\n\nLet us see the number of machine failures for each of the machine types.\n\nfacet_lookup &lt;- c(\"H\" = \"Heavy (H)\",\"L\"= \"Light (L)\",\"M\" = \"Medium (M)\")\ndf_type_group &lt;- df_train %&gt;% group_by(type,machine_failure) %&gt;% summarise(count = n())\ndf_type_group &lt;- df_type_group %&gt;% group_by(type) %&gt;% mutate(total = sum(count))\ndf_type_group &lt;- df_type_group %&gt;% mutate(percentage = 100 * (count/total))\npl1 &lt;- ggplot(data = df_type_group, \n              aes(x = factor(machine_failure),\n                  y = count)) + geom_col(aes(fill = type),color='black') + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup)) + geom_label(aes(label = count)) + labs(x = \"Machine failure status\", \n                                                                                                                       y = \"Number of incidents\") + ggtitle(\"Number of machine failures for each type\") + theme_classic() + theme(legend.position = 'none') \npl1\n\n\n\n\nNumber of machine failures for each type\n\n\n\n\nFigure @ref(fig:failtype) illustrates the number of failures observed for each machine type. The failures constitute:\n- 1 % of the incidents for machine type ‚ÄúH‚Äù\n- 2 % of the incidents for machine type ‚ÄúL‚Äù\n- 1 % of the incidents for machine type ‚ÄúM‚Äù\n\n\n üìµ Hence, we can observe that the number of failure cases are fairly evenly distributed among each of the machine types. üìµ ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#label2",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#label2",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Air and process temperatures",
    "text": "Air and process temperatures\nTemperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).\nLeet us first study the distribution of the temperature values.\n\npl2 &lt;- ggplot(data = df_train,\n              aes(x = process_temperature_k,\n                  fill = factor(hdf)),\n              alpha = 0.6) + geom_histogram(alpha = 0.8,\n                                            position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Process temperature (K)\" , y = \"Number of incidences\", fill = 'HDF status')\n\npl3 &lt;- ggplot(data = df_train,\n              aes(x = air_temperature_k,\n                  fill = factor(hdf))) + geom_histogram(alpha = 0.8,\n                                                        position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Air temperature (K)\" , y = \"Number of incidences\", fill = 'HDF status')\n\nplot_grid(pl3,pl2, labels = c(\"Air temperatures\",\"Process temperatures\"))\n\n\n\n\nTemperature variation distribution\n\n\n\n\nAs we can observe from @ref(fig:temps), majority of the heat dissipation failures have occurred at relatively higher values of air temperatures. These air temperatures are observed to be around 302.5 K. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.\nHeat dissipation values are governed by the following heat transfer equation.\n\\[ \\boxed{\\Delta H = mC_p(T_{process} - T_{air})}\\] Based on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.\n\ndf_temp_diff &lt;- df_train %&gt;% select(c(process_temperature_k,air_temperature_k,hdf)) %&gt;% mutate(temp_diff = process_temperature_k - air_temperature_k)\n\n\npl4 &lt;- ggplot(data = df_temp_diff,\n              aes(x = temp_diff,\n                  fill = factor(hdf)),\n              alpha = 0.6) + geom_histogram(alpha = 0.8,\n                                            position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Temperature difference (K)\" , y = \"Number of incidences\", fill = 'HDF status') + ggtitle(\"Heat dissipation failure based on temperature difference\") + \n  \n  annotate(\"segment\",x = 5,\n    y = 2500,xend = 7 ,\n    yend = 5 ,arrow = arrow(type = \"closed\", \n                              length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\"text\",x = 5,\n    y = 3800,colour = \"red\",\n    label = 'High chances of HDF \\n due to low temperature difference',\n    size = unit(3, \"pt\")) + theme(axis.text.x = element_text(angle = 10,face = 'bold')) \npl4\n\n\n\n\nHeat dissipation failure based on temperature difference\n\n\n\n\nAs illustrated by figure @ref(fig:tempdiff) and based on the heat transfer equation, we can observe that\n\n ‚ö†Ô∏è the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures. ‚ö†Ô∏è  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#label1",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#label1",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Torque and Tool rotation speed",
    "text": "Torque and Tool rotation speed\nThe torque of a machine can be defined as the amount of rotational energy required to perform mechanical work. As a result of torque applied, a machine element, such as the tool in this case rotates at a particular speed. This speed of rotation is measured by the tool rotation speed in revolutions per minute (RPM).\nIn addition to the above definitions, the product of the torque and the tool rotation speeds give us the value of the power consumption of a machine. The equaiton for the same is as follows :\n\\[ \\boxed{P = \\omega T} \\] Where,\n\\(P =\\) Power consumption of the machine in Watts\n\\(T =\\) Torque in Nm\n\\(\\omega = 2\\pi N/60\\), with \\(N\\) being the tool rotational speed (RPM)\n\nLet us now visualise the torque and tool rotation speed values for each machine type.\n\npl5 &lt;- ggplot(data = df_train, aes(x = torque_nm , y = rotational_speed_rpm)) + geom_bin_2d() + theme_classic() + labs(x = \"Torque (Nm)\", \n                                                                                                                                      y = \"Rotational speed (RPM)\") +  ggtitle(\"Working window of Torque and Rotational speeds\")\npl5\n\n\n\n\nWorking window of Torque and Rotational speeds\n\n\n\n\nBased on the plot in figure @ref(fig:pwf), we can observe\n\n ‚ö†Ô∏èThe ideal working window for torque lies between 25 Nm - 50 Nm while that for the rotational speed lies between 1250-2000 RPM . ‚ö†Ô∏è  .\n\nLet us now try to study how do the values for tool rotation speeds and torque vary based on power failure (PWF) faceted for each machine type.\n\npl6 &lt;- ggplot(data = df_train, aes(x = torque_nm , \n                                   y = rotational_speed_rpm, \n                                   fill = factor(pwf))) + geom_hex() + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Torque (Nm)\", \n                                                                                               y = \"Rotational speed (RPM)\") + guides(color = FALSE) + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup)) + labs(fill = \"PWF status\") + ggtitle(\"Torque and rotational speed window \\n for each machine type\") + theme(plot.title = element_text(hjust=0.5))\npl6\n\n\n\n\nFaceted hex plot of working windows for each machine type\n\n\n\n\nBased on the analysis of figure @ref(fig:facetwindown) we can conclude that\n\n‚ùå Power failures (PWF) are observed to majorly occur outside the ideal working window. These failures are  majorly concentrated in either the regions of high torque and low rotational speeds or low torque and high rotational speeds. The observation holds consistent for all three machine types ‚ùå  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#power-consumption",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#power-consumption",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Power consumption",
    "text": "Power consumption\nBased on the analysis in section @ref(label1), let us study how does the power consumption differ for machines which have undergone power failure (PWF).\n\ndf_train &lt;- df_train %&gt;% mutate(power_w = torque_nm * 2* pi * rotational_speed_rpm/60)\n\npl7 &lt;- ggplot(data = df_train,\n              aes(x = power_w ,\n                  fill = factor(pwf))) + geom_density(alpha = 0.6) + theme_classic() + scale_fill_manual(values = c(\"#3268a8\",\"#9c1144\")) + labs(x = \"Power consumption (in W)\",y = \"Probability density\",fill = \"PWF status\" ) + ggtitle(\"Probability density of power consumption for each PWF status\") + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup))\npl7\n\n\n\n\nProbability density of power consumption for each PWF status\n\n\n\n\nFigure @ref(fig:power) illustrates the density distribution of power consumption for each power failure (PWF) status and faceted for each of the machine types. We can observe that:\n\n‚ö°Ô∏èthe density plot for machines which have undergone power failure is bimodal in nature while the plot is unimodal for machines which did not undergo power failure. Based on the density plot, we can observe that the ideal working window for power consumption should be between 4000-10000 W. Machines reporting power consumption below or above this band are observed to be prone to undergo power failure.‚ö°Ô∏è"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#toolwear",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#toolwear",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Toolwear",
    "text": "Toolwear\nThe toolwear can play a critical role in terms of overstrain failure (OSF) as it can lead to excess loads on various parts of the machine equipment. Hence, it is pertinent to study the importance of toolwear through visualisations.\n\npl8 &lt;- ggplot(data = df_train, aes(x= factor(twf),y = tool_wear_min, fill = factor(twf))) + geom_violin() + scale_fill_manual(values = c('blue','red')) + geom_boxplot(width=0.1, color=\"black\", alpha=0.2) + theme_classic() + labs(x = 'Overstrain failure (OSF) status',y = \"Tool wear (mm)\") + guides(fill = FALSE) + ggtitle(\"Tool wear values for overstrain failure status (OSF) \\n faceted for each machine type\") + facet_wrap(~type,labeller = as_labeller(facet_lookup))\npl8\n\n\n\n\nTool wear values for overstrain failure status (OSF) faceted for each machine type\n\n\n\n\nAfter analysing figure @ref(fig:toolwear), we observe that\n\nüî© Overstrain failures as a result of tool wear occurs majorly for tool wear values of 200 mm or above . While there are some overstrain failures at low toolwear values, however, OSF is majorly a result of higher toolwear as can be observed through the violin plots. This observation is fairly consistent for each of the three machine types. üî©."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#logistic-regression",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#logistic-regression",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\nüí° Upon studying the McFadden \\(R^2\\) value, we observe that the model accuracy was approximately 71.2%  üí°.\n\nLet us now observe how well we can predict on the test dataset based on the logistic regression model.\n\nfitted.results &lt;- predict(model_logit,newdata=subset(test,select=-(machine_failure)),type='response')\nfitted.results &lt;- ifelse(fitted.results &gt; 0.5,1,0)\n\nmisClasificError &lt;- mean(fitted.results != test$machine_failure)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n\n\nAs we can observe, the logistic regression model was able to accurately predict 97.2% of the machine failures.\nLet us further study the performance of the logistic regression model through the Receiver Operating Curve (ROC) metric.\n\np &lt;- as.numeric(predict(model_logit, newdata=subset(test,select=-c(machine_failure)), type=\"response\"))\npr &lt;- prediction(p, test$machine_failure)\nprf &lt;- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(prf)\nauc_logit &lt;- performance(pr, measure = \"auc\")\nauc_logit &lt;- auc_logit@y.values[[1]]\ntitle(\"Receiver Operating Curve for Logistic Regression\")\n\n\n\n\nReceiver Operating Curve for Logistic Regression\n\n\n\n\nBased on the ROC as illustrated by figure @ref(fig:roc-logit), we can observe that a large section of the upper half of the plot has been covered by the operating curve.\n\nüí° The Area Under Curve (AUC) score of 0.9261326 suggests that the model was able to  predict the machine failures fairly well.  üí°\n\nNext, we try to obtain a more intuitive performance metric of the model by creating a confusion matrix.\n\ndraw_confusion_matrix &lt;- function(cm) {\n\n  layout(matrix(c(1,1,2)))\n  par(mar=c(2,2,2,2))\n  plot(c(100, 345), c(300, 450), type = \"n\", xlab=\"\", ylab=\"\", xaxt='n', yaxt='n')\n  title('CONFUSION MATRIX', cex.main=2)\n\n  # create the matrix \n  rect(150, 430, 240, 370, col='#3F97D0')\n  text(195, 435, 'False', cex=1.2)\n  rect(250, 430, 340, 370, col='#F7AD50')\n  text(295, 435, 'True', cex=1.2)\n  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)\n  text(245, 450, 'Actual', cex=1.3, font=2)\n  rect(150, 305, 240, 365, col='#F7AD50')\n  rect(250, 305, 340, 365, col='#3F97D0')\n  text(140, 400, 'False', cex=1.2, srt=90)\n  text(140, 335, 'True', cex=1.2, srt=90)\n\n  # add in the cm results \n  res &lt;- as.numeric(cm$table)\n  text(195, 400, res[1], cex=1.6, font=2, col='white')\n  text(195, 335, res[2], cex=1.6, font=2, col='white')\n  text(295, 400, res[3], cex=1.6, font=2, col='white')\n  text(295, 335, res[4], cex=1.6, font=2, col='white')\n\n  # add in the specifics \n  plot(c(100, 0), c(100, 0), type = \"n\", xlab=\"\", ylab=\"\", main = \"DETAILS\", xaxt='n', yaxt='n')\n  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)\n  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)\n  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)\n  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)\n  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)\n  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)\n  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)\n  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)\n  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)\n  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)\n\n  # add in the accuracy information \n  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)\n  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)\n  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)\n  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)\n}  \n\n\ncm_logit &lt;- confusionMatrix(factor(fitted.results),factor(test$machine_failure))\ndraw_confusion_matrix(cm_logit)\n\n\n\n\nConfusion matrix for Logistic Regression\n\n\n\n\nFigure @ref(fig:conf-mat) illustrates the confusion matrix for the logistic regression model along with its various performance metrics.\n\ntest_id &lt;- df_test$id\ndf_test &lt;- df_test %&gt;% select(-c(id,`Product ID`))\ndf_test &lt;- clean_names(df_test)\ndf_test$type &lt;- factor(df_test$type)\ndf_test&lt;- data.table(df_test)\ndf_test &lt;- one_hot(df_test,cols = as.factor(\"type\"))\n\ndf_test &lt;- as.data.frame(df_test)\n\ndf_test &lt;- df_test %&gt;% mutate(temp_diff_k = process_temperature_k - air_temperature_k) %&gt;% select(-c(process_temperature_k,air_temperature_k))\ndf_test &lt;- df_test %&gt;% mutate(power_w = torque_nm * 2* pi * rotational_speed_rpm/60) %&gt;% select(-c(torque_nm,rotational_speed_rpm))\n\ndf_test &lt;- df_test %&gt;% select(c(\"type_H\",\"type_L\",\"type_M\",\"tool_wear_min\",\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\",\"power_w\",\"temp_diff_k\"))\n\n\nfitted.results &lt;- predict(model_logit,df_test,type='response')\nfitted.results &lt;- as.data.frame(ifelse(fitted.results &gt; 0.5,1,0))\nfitted.results &lt;- fitted.results %&gt;% rename(\"Machine failure\" = \"ifelse(fitted.results &gt; 0.5, 1, 0)\")\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Logistic_Reg_predictions.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#random-forest",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#random-forest",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nLet us use an ensemble algorithm to classify our results. We shall utilise the Random Forest technique which utilises multiple decision trees to predict results.\n\nrf_model&lt;-randomForest(machine_failure~.,data=train)\n\n\nplot(rf_model)\n\n\n\n\nError vs Number of trees for Random Forest\n\n\n\n\nAs we can observe from figure @ref(fig:errormod),\n\nüí° the error of the random forest model is observed to reduce  as the number of trees cross 100 .  üí°\n\n\npred_rf &lt;- predict(rf_model,test, type = 'class')\npred_rf &lt;- if_else(pred_rf &gt; 0.3,1,0)\ncm_rf &lt;- confusionMatrix(factor(pred_rf),factor(test$machine_failure))\ndraw_confusion_matrix(cm_rf)\n\n\n\n\nConfusion matrix of the random forest model\n\n\n\n\n\nfitted.results &lt;- predict(rf_model,df_test,type='class')\nfitted.results &lt;- as.data.frame(ifelse(fitted.results &gt; 0.3,1,0))\nfitted.results &lt;- fitted.results %&gt;% rename(\"Machine failure\" = \"ifelse(fitted.results &gt; 0.3, 1, 0)\")\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"RF_predictions.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#xgboost",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#xgboost",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "XGboost",
    "text": "XGboost\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\nxgb_model &lt;- xgboost(data = as.matrix(train %&gt;% select(-c(machine_failure))), label = as.matrix(train$machine_failure), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n\n[1] train-logloss:0.144407 \n[2] train-logloss:0.059787 \n\n\n\npred_xgb &lt;- predict(xgb_model, as.matrix(test %&gt;% select(-c(machine_failure))))\npred_xgb &lt;- if_else(pred_xgb &gt; 0.3,1,0)\n\n\ncm_xgb &lt;- confusionMatrix(factor(pred_xgb),factor(test$machine_failure))\ndraw_confusion_matrix(cm_xgb)\n\n\n\n\nConfusion matrix of the XGBoost model\n\n\n\n\n\npreds &lt;- predict(xgb_model,as.matrix(df_test))\nfitted.results &lt;- as.data.frame(preds)\nfitted.results &lt;- fitted.results %&gt;% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\n\nwrite_csv(fitted.results,\"XGB_predictions.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#light-gbm",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#light-gbm",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Light GBM",
    "text": "Light GBM\nLet us utilise the LGBM algorithm and train it on the given dataset.\n\npred_lgb = predict(model_lgb, as.matrix(test))\npred_lgb &lt;- if_else(pred_lgb &gt; 0.3,1,0)\ncm_lgb &lt;- confusionMatrix(factor(pred_lgb),factor(test$machine_failure))\ndraw_confusion_matrix(cm_lgb)\n\n\n\n\nConfusion matrix of the Light GB model\n\n\n\n\n\npreds &lt;- predict(model_lgb,as.matrix(df_test),predict_disable_shape_check=TRUE)\nfitted.results &lt;- as.data.frame(preds)\nfitted.results &lt;- fitted.results %&gt;% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"LGBM_predictions_std.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#catboost",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#catboost",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Catboost",
    "text": "Catboost\n\nimportance &lt;- varImp(model_cat, scale = FALSE)\nplot(importance)\n\n\n\n\nFeature importance using Catboost model\n\n\n\n\n\npred_cat = predict(model_cat, test %&gt;% select(-machine_failure))\npred_cat &lt;- if_else(pred_cat == \"X0\",0,1)\ncm_cat &lt;- confusionMatrix(factor(pred_cat),factor(test$machine_failure))\ndraw_confusion_matrix(cm_cat)\n\n\n\n\nConfusion matrix of the Catboost model\n\n\n\n\n\nfitted.results &lt;- predict(model_cat,df_test)\nfitted.results &lt;- as.data.frame(ifelse(fitted.results == \"X0\",0,1))\nfitted.results &lt;- fitted.results %&gt;% rename_with(.cols = 1, ~\"Machine failure\")\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Cat_predictions.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#feature-transformation",
    "href": "posts/blogs/Machine_failure_prediction/Machine_failure_preds.html#feature-transformation",
    "title": "üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction",
    "section": "Feature transformation",
    "text": "Feature transformation\nNow that we have created all our baseline models, let us try our hand out with some feature transformation with standard scaling options.\n\ntrain$tool_wear_min &lt;- train$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$power_w &lt;- train$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$temp_diff_k &lt;- train$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\ntest$tool_wear_min &lt;- test$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntest$power_w &lt;- test$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntest$temp_diff_k &lt;- test$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\n\ncols &lt;- c(\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\")\n\n\ntrain %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)   #Converting  to factors\ntest %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\ndf_test %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\n\nNow that we have standardised all the continuous numeric variables, let us attempt to train the model once again on the scaled dataset.\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\nfitted.results &lt;- predict(model_logit,newdata=subset(test,select=-(machine_failure)),type='response')\nfitted.results &lt;- ifelse(fitted.results &gt; 0.5,1,0)\n\nmisClasificError &lt;- mean(fitted.results != test$machine_failure)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n\n\n\ndf_test$tool_wear_min &lt;- df_test$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ndf_test$power_w &lt;- df_test$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ndf_test$temp_diff_k &lt;- df_test$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\n\npreds &lt;- predict(model_logit,df_test,type='response')\nfitted.results &lt;- as.data.frame(preds)\nfitted.results &lt;- fitted.results %&gt;% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id &lt;- test_id\nfitted.results &lt;- fitted.results %&gt;% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Logistic_Reg_predictions_std.csv\")"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "",
    "text": "Binary machine failure prediction using machine learning is a technique employed to anticipate the occurrence of failures or malfunctions in a binary system or machine. With the increasing complexity of modern machines, the ability to predict and prevent failures becomes crucial for optimizing performance, reducing downtime, and avoiding costly repairs.\nMachine learning algorithms play a vital role in this prediction process by analyzing historical data and identifying patterns or anomalies that indicate potential failures. These algorithms learn from past failure instances, considering various factors such as sensor readings, environmental conditions, maintenance records, and other relevant parameters.\nThe predictive models are trained on labeled datasets, where each instance is associated with a failure or non-failure outcome. Common machine learning techniques used for binary machine failure prediction include logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks.\nDuring the training phase, the algorithms learn the relationships between input features and failure occurrences, thereby enabling them to make accurate predictions on unseen data. Feature engineering, which involves selecting or transforming relevant input variables, is an essential step in improving the model‚Äôs performance.\nOnce the model is trained, it can be deployed to make real-time predictions on new data streams. By continuously monitoring machine inputs and comparing them to the learned patterns, the system can generate alerts or take preventive actions whenever a potential failure is detected. This proactive approach helps minimize unexpected downtime, reduce maintenance costs, and improve overall operational efficiency.\nBinary machine failure prediction using machine learning is widely applied across various industries, including manufacturing, power generation, healthcare, transportation, and more. By leveraging the power of data and advanced analytics, it offers a valuable tool for optimizing maintenance strategies, enhancing productivity, and ensuring the reliability of critical systems.\n\n\n\n\n\nFigure¬†1: Source: www.gesrepair.com"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#check-for-null-values",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#check-for-null-values",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.1 Check for null values",
    "text": "3.1 Check for null values\nAs a part of checking for the cleanliness of the dataset, let us visaulise the presence of null values for each of the variables.\n\n\n\n\n\nFigure¬†2: Missingness in the dataset\n\n\n\n\nAs we can observe from figure Figure¬†2, there are no missing values for any of the variables in the dataset. As a result, the dataset can be considered clean for further analysis."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#removal-of-variables",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#removal-of-variables",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.2 Removal of variables",
    "text": "3.2 Removal of variables\nAfter studying for the presence of null values, we now remove the variables that do not provide any extra insights into our analysis.\n\ndf_train &lt;- df_train %&gt;% select(-c(id,`Product ID`))"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#cleaning-the-variable-names",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#cleaning-the-variable-names",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.3 Cleaning the variable names",
    "text": "3.3 Cleaning the variable names\nThe current dataset contains variable names which are not ideal for data wrangling and EDA. Hence, we will try to remove any unnecessary white space and special characters for each of the variable names.\n\ndf_train &lt;- clean_names(df_train)\nhead(df_train)\n\n# A tibble: 6 √ó 12\n  type  air_temperature_k process_temperature_k rotational_speed_rpm torque_nm\n  &lt;chr&gt;             &lt;dbl&gt;                 &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 L                  301.                  310.                 1596      36.1\n2 M                  303.                  312.                 1759      29.1\n3 L                  299.                  308.                 1805      26.5\n4 L                  301                   311.                 1524      44.3\n5 M                  298                   309                  1641      35.4\n6 M                  298.                  309.                 1429      42.1\n# ‚Ñπ 7 more variables: tool_wear_min &lt;dbl&gt;, machine_failure &lt;dbl&gt;, twf &lt;dbl&gt;,\n#   hdf &lt;dbl&gt;, pwf &lt;dbl&gt;, osf &lt;dbl&gt;, rnf &lt;dbl&gt;"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#type-of-machine",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#type-of-machine",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.1 Type of machine",
    "text": "4.1 Type of machine\nThere are a total of 3 types machines in this dataset. These are encoded as:\n\nL (Light)\nM (Medium)\nH (Heavy)\n\nLet us see the number of machine failures for each of the machine types.\n\n\n\n\n\nFigure¬†3: Number of machine failures for each type\n\n\n\n\nFigure Figure¬†3 illustrates the number of failures observed for each machine type. The failures constitute:\n- 1 % of the incidents for machine type ‚ÄúH‚Äù\n- 2 % of the incidents for machine type ‚ÄúL‚Äù\n- 1 % of the incidents for machine type ‚ÄúM‚Äù\n\n\nüìµ  Hence, we can observe that the number of failure cases are fairly evenly distributed among each of the machine types.  üìµ ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#label2",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#label2",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.2 Air and process temperatures",
    "text": "4.2 Air and process temperatures\nTemperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).\nLeet us first study the distribution of the temperature values.\n\n\n\n\n\nTemperature variation distribution\n\n\n\n\nAs we can observe from @ref(fig:temps), majority of the heat dissipation failures have occurred at relatively higher values of air temperatures. These air temperatures are observed to be around 302.5 K. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.\nHeat dissipation values are governed by the following heat transfer equation.\n\\[ \\boxed{\\Delta H = mC_p(T_{process} - T_{air})}\\] Based on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.\n\n\n\n\n\nHeat dissipation failure based on temperature difference\n\n\n\n\nAs illustrated by figure @ref(fig:tempdiff) and based on the heat transfer equation, we can observe that\n\n ‚ö†Ô∏è the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures. ‚ö†Ô∏è  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#label1",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#label1",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.3 Torque and Tool rotation speed",
    "text": "4.3 Torque and Tool rotation speed\nThe torque of a machine can be defined as the amount of rotational energy required to perform mechanical work. As a result of torque applied, a machine element, such as the tool in this case rotates at a particular speed. This speed of rotation is measured by the tool rotation speed in revolutions per minute (RPM).\nIn addition to the above definitions, the product of the torque and the tool rotation speeds give us the value of the power consumption of a machine. The equaiton for the same is as follows :\n\\[ \\boxed{P = \\omega T} \\] Where,\n\\(P =\\) Power consumption of the machine in Watts\n\\(T =\\) Torque in Nm\n\\(\\omega = 2\\pi N/60\\), with \\(N\\) being the tool rotational speed (RPM)\n\nLet us now visualise the torque and tool rotation speed values for each machine type.\n\n\n\n\n\nWorking window of Torque and Rotational speeds\n\n\n\n\nBased on the plot in figure @ref(fig:pwf), we can observe\n\n ‚ö†Ô∏èThe ideal working window for torque lies between 25 Nm - 50 Nm while that for the rotational speed lies between 1250-2000 RPM . ‚ö†Ô∏è  .\n\nLet us now try to study how do the values for tool rotation speeds and torque vary based on power failure (PWF) faceted for each machine type.\n\n\n\n\n\nFaceted hex plot of working windows for each machine type\n\n\n\n\nBased on the analysis of figure @ref(fig:facetwindown) we can conclude that\n\n‚ùå Power failures (PWF) are observed to majorly occur outside the ideal working window. These failures are  majorly concentrated in either the regions of high torque and low rotational speeds or low torque and high rotational speeds. The observation holds consistent for all three machine types ‚ùå  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#power-consumption",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#power-consumption",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.4 Power consumption",
    "text": "4.4 Power consumption\nBased on the analysis in section Section¬†4.3, let us study how does the power consumption differ for machines which have undergone power failure (PWF).\n\n\n\n\n\nFigure¬†8: Probability density of power consumption for each PWF status\n\n\n\n\nFigure Figure¬†8 illustrates the density distribution of power consumption for each power failure (PWF) status and faceted for each of the machine types. We can observe that:\n\n‚ö°Ô∏èthe density plot for machines which have undergone power failure is bimodal in nature while the plot is unimodal for machines which did not undergo power failure. Based on the density plot, we can observe that the ideal working window for power consumption should be between 4000-10000 W. Machines reporting power consumption below or above this band are observed to be prone to undergo power failure.‚ö°Ô∏è"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#toolwear",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#toolwear",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.5 Toolwear",
    "text": "4.5 Toolwear\nThe toolwear can play a critical role in terms of overstrain failure (OSF) as it can lead to excess loads on various parts of the machine equipment. Hence, it is pertinent to study the importance of toolwear through visualisations.\n\n\n\n\n\nFigure¬†9: Tool wear values for overstrain failure status (OSF) faceted for each machine type\n\n\n\n\nAfter analysing figure Figure¬†9, we observe that\n\nüî© Overstrain failures as a result of tool wear occurs  majorly for tool wear values of 200 mm or above . While there are some overstrain failures at low toolwear values, however, OSF is majorly a result of higher toolwear as can be observed through the violin plots. This observation is fairly consistent for each of the three machine types. üî©."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#logistic-regression",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#logistic-regression",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.1 Logistic Regression",
    "text": "7.1 Logistic Regression\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\nüí° Upon studying the McFadden \\(R^2\\) value, we observe that the model accuracy was approximately 71.2%  üí°.\n\nLet us now observe how well we can predict on the test dataset based on the logistic regression model.\n\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n\n\nAs we can observe, the logistic regression model was able to accurately predict 97.2% of the machine failures.\nLet us further study the performance of the logistic regression model through the Receiver Operating Curve (ROC) metric.\n\n\n\n\n\nFigure¬†11: Receiver Operating Curve for Logistic Regression\n\n\n\n\nBased on the ROC as illustrated by figure Figure¬†11, we can observe that a large section of the upper half of the plot has been covered by the operating curve.\n\nüí° The Area Under Curve (AUC) score of 0.9261326 suggests that the model was able to  predict the machine failures fairly well.  üí°\n\nNext, we try to obtain a more intuitive performance metric of the model by creating a confusion matrix.\n\n\n\n\n\nFigure¬†12: Confusion matrix for Logistic Regression\n\n\n\n\nFigure Figure¬†12 illustrates the confusion matrix for the logistic regression model along with its various performance metrics."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#random-forest",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#random-forest",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.2 Random Forest",
    "text": "7.2 Random Forest\nLet us use an ensemble algorithm to classify our results. We shall utilise the Random Forest technique which utilises multiple decision trees to predict results.\n\n\n\n\n\nFigure¬†13: Error vs Number of trees for Random Forest\n\n\n\n\nAs we can observe from figure Figure¬†13,\n\nüí° the error of the random forest model is observed to reduce  as the number of trees cross 100 . üí°\n\n\n\n\n\n\nFigure¬†14: Confusion matrix of the random forest model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#xgboost",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#xgboost",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.3 XGboost",
    "text": "7.3 XGboost\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\n\n[1] train-logloss:0.144407 \n[2] train-logloss:0.059787 \n\n\n\n\n\n\n\nConfusion matrix of the XGBoost model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#light-gbm",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#light-gbm",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.4 Light GBM",
    "text": "7.4 Light GBM\nLet us utilise the LGBM algorithm and train it on the given dataset.\n\n\n\n\n\nConfusion matrix of the Light GB model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#catboost",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#catboost",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.5 Catboost",
    "text": "7.5 Catboost\n\n\n\n\n\nFeature importance using Catboost model\n\n\n\n\n\n\n\n\n\nConfusion matrix of the Catboost model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#feature-transformation",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#feature-transformation",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.6 Feature transformation",
    "text": "7.6 Feature transformation\nNow that we have created all our baseline models, let us try our hand out with some feature transformation with standard scaling options.\n\ntrain$tool_wear_min &lt;- train$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$power_w &lt;- train$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$temp_diff_k &lt;- train$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\ntest$tool_wear_min &lt;- test$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntest$power_w &lt;- test$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntest$temp_diff_k &lt;- test$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\n\ncols &lt;- c(\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\")\n\n\ntrain %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)   #Converting  to factors\ntest %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\ndf_test %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\n\nNow that we have standardised all the continuous numeric variables, let us attempt to train the model once again on the scaled dataset.\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\""
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#sec-label2",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#sec-label2",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.2 Air and process temperatures",
    "text": "4.2 Air and process temperatures\nTemperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).\nLet us first study the distribution of the temperature values.\n\n\n\n\n\nFigure¬†4: Temperature variation distribution\n\n\n\n\nAs we can observe from Figure¬†4, majority of the heat dissipation failures have occurred at relatively higher values of air temperatures. These air temperatures are observed to be around 302.5 K. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.\nHeat dissipation values are governed by the following heat transfer equation.\n\\[ \\boxed{\\Delta H = mC_p(T_{process} - T_{air})}\\] Based on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.\n\n\n\n\n\nFigure¬†5: Heat dissipation failure based on temperature difference\n\n\n\n\nAs illustrated by figure Figure¬†5 and based on the heat transfer equation, we can observe that\n\n ‚ö†Ô∏è the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures. ‚ö†Ô∏è  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#sec-label1",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#sec-label1",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.3 Torque and Tool rotation speed",
    "text": "4.3 Torque and Tool rotation speed\nThe torque of a machine can be defined as the amount of rotational energy required to perform mechanical work. As a result of torque applied, a machine element, such as the tool in this case rotates at a particular speed. This speed of rotation is measured by the tool rotation speed in revolutions per minute (RPM).\nIn addition to the above definitions, the product of the torque and the tool rotation speeds give us the value of the power consumption of a machine. The equaiton for the same is as follows :\n\\[ \\boxed{P = \\omega T} \\] Where,\n\\(P =\\) Power consumption of the machine in Watts\n\\(T =\\) Torque in Nm\n\\(\\omega = 2\\pi N/60\\), with \\(N\\) being the tool rotational speed (RPM)\n\nLet us now visualise the torque and tool rotation speed values for each machine type.\n\n\n\n\n\nFigure¬†6: Working window of Torque and Rotational speeds\n\n\n\n\nBased on the plot in figure Figure¬†6, we can observe\n\n ‚ö†Ô∏èThe ideal working window for torque lies between 25 Nm - 50 Nm while that for the rotational speed lies between 1250-2000 RPM . ‚ö†Ô∏è  .\n\nLet us now try to study how do the values for tool rotation speeds and torque vary based on power failure (PWF) faceted for each machine type.\n\n\n\n\n\nFigure¬†7: Faceted hex plot of working windows for each machine type\n\n\n\n\nBased on the analysis of figure Figure¬†7 we can conclude that\n\n‚ùå Power failures (PWF) are observed to majorly occur outside the ideal working window. These failures are  majorly concentrated in either the regions of high torque and low rotational speeds or low torque and high rotational speeds. The observation holds consistent for all three machine types ‚ùå  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/machine_failure.html#feature-transformation-and-model-implementation",
    "href": "posts/blogs/Machine_failure_prediction/machine_failure.html#feature-transformation-and-model-implementation",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.6 Feature transformation and model implementation",
    "text": "7.6 Feature transformation and model implementation\nNow that we have created all our baseline models, let us try our hand out with some feature transformation with standard scaling options.\n\ntrain$tool_wear_min &lt;- train$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$power_w &lt;- train$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$temp_diff_k &lt;- train$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\ntest$tool_wear_min &lt;- test$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntest$power_w &lt;- test$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntest$temp_diff_k &lt;- test$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\n\ncols &lt;- c(\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\")\n\n\ntrain %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)   #Converting  to factors\ntest %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\ndf_test %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\n\nNow that we have standardised all the continuous numeric variables, let us attempt to train the model once again on the scaled dataset.\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\""
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html",
    "href": "posts/blogs/Machine_failure_prediction/index.html",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "",
    "text": "Binary machine failure prediction using machine learning is a technique employed to anticipate the occurrence of failures or malfunctions in a binary system or machine. With the increasing complexity of modern machines, the ability to predict and prevent failures becomes crucial for optimizing performance, reducing downtime, and avoiding costly repairs.\nMachine learning algorithms play a vital role in this prediction process by analyzing historical data and identifying patterns or anomalies that indicate potential failures. These algorithms learn from past failure instances, considering various factors such as sensor readings, environmental conditions, maintenance records, and other relevant parameters.\nThe predictive models are trained on labeled datasets, where each instance is associated with a failure or non-failure outcome. Common machine learning techniques used for binary machine failure prediction include logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks.\nDuring the training phase, the algorithms learn the relationships between input features and failure occurrences, thereby enabling them to make accurate predictions on unseen data. Feature engineering, which involves selecting or transforming relevant input variables, is an essential step in improving the model‚Äôs performance.\nOnce the model is trained, it can be deployed to make real-time predictions on new data streams. By continuously monitoring machine inputs and comparing them to the learned patterns, the system can generate alerts or take preventive actions whenever a potential failure is detected. This proactive approach helps minimize unexpected downtime, reduce maintenance costs, and improve overall operational efficiency.\nBinary machine failure prediction using machine learning is widely applied across various industries, including manufacturing, power generation, healthcare, transportation, and more. By leveraging the power of data and advanced analytics, it offers a valuable tool for optimizing maintenance strategies, enhancing productivity, and ensuring the reliability of critical systems.\n\n\n\n\n\nFigure¬†1: Source: www.gesrepair.com"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#check-for-null-values",
    "href": "posts/blogs/Machine_failure_prediction/index.html#check-for-null-values",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.1 Check for null values",
    "text": "3.1 Check for null values\nAs a part of checking for the cleanliness of the dataset, let us visaulise the presence of null values for each of the variables.\n\n\n\n\n\nFigure¬†2: Missingness in the dataset\n\n\n\n\nAs we can observe from figure Figure¬†2, there are no missing values for any of the variables in the dataset. As a result, the dataset can be considered clean for further analysis."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#removal-of-variables",
    "href": "posts/blogs/Machine_failure_prediction/index.html#removal-of-variables",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.2 Removal of variables",
    "text": "3.2 Removal of variables\nAfter studying for the presence of null values, we now remove the variables that do not provide any extra insights into our analysis.\n\ndf_train &lt;- df_train %&gt;% select(-c(id,`Product ID`))"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#cleaning-the-variable-names",
    "href": "posts/blogs/Machine_failure_prediction/index.html#cleaning-the-variable-names",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "3.3 Cleaning the variable names",
    "text": "3.3 Cleaning the variable names\nThe current dataset contains variable names which are not ideal for data wrangling and EDA. Hence, we will try to remove any unnecessary white space and special characters for each of the variable names.\n\ndf_train &lt;- clean_names(df_train)\nhead(df_train)\n\n# A tibble: 6 √ó 12\n  type  air_temperature_k process_temperature_k rotational_speed_rpm torque_nm\n  &lt;chr&gt;             &lt;dbl&gt;                 &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 L                  301.                  310.                 1596      36.1\n2 M                  303.                  312.                 1759      29.1\n3 L                  299.                  308.                 1805      26.5\n4 L                  301                   311.                 1524      44.3\n5 M                  298                   309                  1641      35.4\n6 M                  298.                  309.                 1429      42.1\n# ‚Ñπ 7 more variables: tool_wear_min &lt;dbl&gt;, machine_failure &lt;dbl&gt;, twf &lt;dbl&gt;,\n#   hdf &lt;dbl&gt;, pwf &lt;dbl&gt;, osf &lt;dbl&gt;, rnf &lt;dbl&gt;"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#type-of-machine",
    "href": "posts/blogs/Machine_failure_prediction/index.html#type-of-machine",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.1 Type of machine",
    "text": "4.1 Type of machine\nThere are a total of 3 types machines in this dataset. These are encoded as:\n\nL (Light)\nM (Medium)\nH (Heavy)\n\nLet us see the number of machine failures for each of the machine types.\n\n\n\n\n\nFigure¬†3: Number of machine failures for each type\n\n\n\n\nFigure Figure¬†3 illustrates the number of failures observed for each machine type. The failures constitute:\n- 1 % of the incidents for machine type ‚ÄúH‚Äù\n- 2 % of the incidents for machine type ‚ÄúL‚Äù\n- 1 % of the incidents for machine type ‚ÄúM‚Äù\n\n\nüìµ  Hence, we can observe that the number of failure cases are fairly evenly distributed among each of the machine types.  üìµ ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#sec-label2",
    "href": "posts/blogs/Machine_failure_prediction/index.html#sec-label2",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.2 Air and process temperatures",
    "text": "4.2 Air and process temperatures\nTemperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).\nLet us first study the distribution of the temperature values.\n\n\n\n\n\nFigure¬†4: Temperature variation distribution\n\n\n\n\nAs we can observe from Figure¬†4, majority of the heat dissipation failures have occurred at relatively higher values of air temperatures. These air temperatures are observed to be around 302.5 K. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.\nHeat dissipation values are governed by the following heat transfer equation.\n\\[ \\boxed{\\Delta H = mC_p(T_{process} - T_{air})}\\] Based on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.\n\n\n\n\n\nFigure¬†5: Heat dissipation failure based on temperature difference\n\n\n\n\nAs illustrated by figure Figure¬†5 and based on the heat transfer equation, we can observe that\n\n ‚ö†Ô∏è the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures. ‚ö†Ô∏è  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#sec-label1",
    "href": "posts/blogs/Machine_failure_prediction/index.html#sec-label1",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.3 Torque and Tool rotation speed",
    "text": "4.3 Torque and Tool rotation speed\nThe torque of a machine can be defined as the amount of rotational energy required to perform mechanical work. As a result of torque applied, a machine element, such as the tool in this case rotates at a particular speed. This speed of rotation is measured by the tool rotation speed in revolutions per minute (RPM).\nIn addition to the above definitions, the product of the torque and the tool rotation speeds give us the value of the power consumption of a machine. The equaiton for the same is as follows :\n\\[ \\boxed{P = \\omega T} \\] Where,\n\\(P =\\) Power consumption of the machine in Watts\n\\(T =\\) Torque in Nm\n\\(\\omega = 2\\pi N/60\\), with \\(N\\) being the tool rotational speed (RPM)\n\nLet us now visualise the torque and tool rotation speed values for each machine type.\n\n\n\n\n\nFigure¬†6: Working window of Torque and Rotational speeds\n\n\n\n\nBased on the plot in figure Figure¬†6, we can observe\n\n ‚ö†Ô∏èThe ideal working window for torque lies between 25 Nm - 50 Nm while that for the rotational speed lies between 1250-2000 RPM . ‚ö†Ô∏è  .\n\nLet us now try to study how do the values for tool rotation speeds and torque vary based on power failure (PWF) faceted for each machine type.\n\n\n\n\n\nFigure¬†7: Faceted hex plot of working windows for each machine type\n\n\n\n\nBased on the analysis of figure Figure¬†7 we can conclude that\n\n‚ùå Power failures (PWF) are observed to majorly occur outside the ideal working window. These failures are  majorly concentrated in either the regions of high torque and low rotational speeds or low torque and high rotational speeds. The observation holds consistent for all three machine types ‚ùå  ."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#power-consumption",
    "href": "posts/blogs/Machine_failure_prediction/index.html#power-consumption",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.4 Power consumption",
    "text": "4.4 Power consumption\nBased on the analysis in section Section¬†4.3, let us study how does the power consumption differ for machines which have undergone power failure (PWF).\n\n\n\n\n\nFigure¬†8: Probability density of power consumption for each PWF status\n\n\n\n\nFigure Figure¬†8 illustrates the density distribution of power consumption for each power failure (PWF) status and faceted for each of the machine types. We can observe that:\n\n‚ö°Ô∏èthe density plot for machines which have undergone power failure is bimodal in nature while the plot is unimodal for machines which did not undergo power failure. Based on the density plot, we can observe that the ideal working window for power consumption should be between 4000-10000 W. Machines reporting power consumption below or above this band are observed to be prone to undergo power failure.‚ö°Ô∏è"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#toolwear",
    "href": "posts/blogs/Machine_failure_prediction/index.html#toolwear",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "4.5 Toolwear",
    "text": "4.5 Toolwear\nThe toolwear can play a critical role in terms of overstrain failure (OSF) as it can lead to excess loads on various parts of the machine equipment. Hence, it is pertinent to study the importance of toolwear through visualisations.\n\n\n\n\n\nFigure¬†9: Tool wear values for overstrain failure status (OSF) faceted for each machine type\n\n\n\n\nAfter analysing figure Figure¬†9, we observe that\n\nüî© Overstrain failures as a result of tool wear occurs  majorly for tool wear values of 200 mm or above . While there are some overstrain failures at low toolwear values, however, OSF is majorly a result of higher toolwear as can be observed through the violin plots. This observation is fairly consistent for each of the three machine types. üî©."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#logistic-regression",
    "href": "posts/blogs/Machine_failure_prediction/index.html#logistic-regression",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.1 Logistic Regression",
    "text": "7.1 Logistic Regression\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\nüí° Upon studying the McFadden \\(R^2\\) value, we observe that the model accuracy was approximately 71.2%  üí°.\n\nLet us now observe how well we can predict on the test dataset based on the logistic regression model.\n\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n\n\nAs we can observe, the logistic regression model was able to accurately predict 97.2% of the machine failures.\nLet us further study the performance of the logistic regression model through the Receiver Operating Curve (ROC) metric.\n\n\n\n\n\nFigure¬†11: Receiver Operating Curve for Logistic Regression\n\n\n\n\nBased on the ROC as illustrated by figure Figure¬†11, we can observe that a large section of the upper half of the plot has been covered by the operating curve.\n\nüí° The Area Under Curve (AUC) score of 0.9261326 suggests that the model was able to  predict the machine failures fairly well.  üí°\n\nNext, we try to obtain a more intuitive performance metric of the model by creating a confusion matrix.\n\n\n\n\n\nFigure¬†12: Confusion matrix for Logistic Regression\n\n\n\n\nFigure Figure¬†12 illustrates the confusion matrix for the logistic regression model along with its various performance metrics."
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#random-forest",
    "href": "posts/blogs/Machine_failure_prediction/index.html#random-forest",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.2 Random Forest",
    "text": "7.2 Random Forest\nLet us use an ensemble algorithm to classify our results. We shall utilise the Random Forest technique which utilises multiple decision trees to predict results.\n\n\n\n\n\nFigure¬†13: Error vs Number of trees for Random Forest\n\n\n\n\nAs we can observe from figure Figure¬†13,\n\nüí° the error of the random forest model is observed to reduce  as the number of trees cross 100 . üí°\n\n\n\n\n\n\nFigure¬†14: Confusion matrix of the random forest model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#xgboost",
    "href": "posts/blogs/Machine_failure_prediction/index.html#xgboost",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.3 XGboost",
    "text": "7.3 XGboost\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\n\n[1] train-logloss:0.144407 \n[2] train-logloss:0.059787 \n\n\n\n\n\n\n\nConfusion matrix of the XGBoost model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#light-gbm",
    "href": "posts/blogs/Machine_failure_prediction/index.html#light-gbm",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.4 Light GBM",
    "text": "7.4 Light GBM\nLet us utilise the LGBM algorithm and train it on the given dataset.\n\n\n\n\n\nConfusion matrix of the Light GB model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#catboost",
    "href": "posts/blogs/Machine_failure_prediction/index.html#catboost",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.5 Catboost",
    "text": "7.5 Catboost\n\n\n\n\n\nFeature importance using Catboost model\n\n\n\n\n\n\n\n\n\nConfusion matrix of the Catboost model"
  },
  {
    "objectID": "posts/blogs/Machine_failure_prediction/index.html#feature-transformation-and-model-implementation",
    "href": "posts/blogs/Machine_failure_prediction/index.html#feature-transformation-and-model-implementation",
    "title": "‚öôÔ∏è‚ö†Ô∏è Can we accurately predict if a machine is going to undergo failure using machine learning ?",
    "section": "7.6 Feature transformation and model implementation",
    "text": "7.6 Feature transformation and model implementation\nNow that we have created all our baseline models, let us try our hand out with some feature transformation with standard scaling options.\n\ntrain$tool_wear_min &lt;- train$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$power_w &lt;- train$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntrain$temp_diff_k &lt;- train$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\ntest$tool_wear_min &lt;- test$tool_wear_min %&gt;% scale(center=TRUE,scale=TRUE)\ntest$power_w &lt;- test$power_w %&gt;% scale(center=TRUE,scale=TRUE)\ntest$temp_diff_k &lt;- test$temp_diff_k %&gt;% scale(center=TRUE,scale=TRUE)\n\n\ncols &lt;- c(\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\")\n\n\ntrain %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)   #Converting  to factors\ntest %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\ndf_test %&lt;&gt;%\n       mutate_each_(funs(factor(.)),cols)\n\nNow that we have standardised all the continuous numeric variables, let us attempt to train the model once again on the scaled dataset.\n\nmodel_logit &lt;- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n\n\n\n\n[1] \"Accuracy of logistic regression: 0.996090695856138\""
  }
]