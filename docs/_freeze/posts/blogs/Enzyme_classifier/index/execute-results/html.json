{
  "hash": "7418548bfb092ef482abacd4770b7ca7",
  "result": {
    "markdown": "---\ntitle: \"Enzyme classifier\"\nimage: \"taipan.jpg\"\ncategories: [data cleaning,exploratory data analysis,visualisation,classification,machine learning]\ndate: \"2024-02-09\"\nformat: html\nbibliography: references.bib\nnumber-sections: true\nexecute: \n  warning: false\n  message: false\n---\n\n\n# Introduction\n\n![](https://y9s5q4c4.stackpathcdn.com/wp-content/uploads/2021/07/enzymology-1.png)\n\n<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>What are we trying to study ?</b></p>\n<p style='margin-left:1em;'>\nThe Enzyme Commission (EC) system is a widely accepted classification system used to categorize enzymes based on their catalytic activities. Enzymes play crucial roles in biological processes by accelerating chemical reactions and facilitating various metabolic pathways within living organisms. The EC system assigns a unique EC number to each enzyme, which provides valuable information about its function and specificity.\n\nEC1 represents the class of enzymes known as oxidoreductases. These enzymes catalyze oxidation-reduction reactions, which involve the transfer of electrons between molecules. Oxidoreductases are involved in a wide range of biological processes, including energy production, biosynthesis, and detoxification. Examples of oxidoreductases include dehydrogenases, oxidases, reductases, and peroxidases.<br> </br>\n    The <b> Enzyme Commission (EC) </b> system provides a systematic and standardized approach for classifying enzymes based on their catalytic activities. EC1 represents the class of oxidoreductases, which participate in oxidation-reduction reactions, while the second digit in the EC number provides more specific information about the enzyme's function. This classification system facilitates the study of enzymes and helps researchers gain insights into their structure, function, and potential applications in various fields, including medicine, biotechnology, and industrial processes.\n</p>\n<p style='margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia'> <b> - The internet </b> <i></i>\n</p></span>\n</div>\n\n\n# Importing the relevant libraries and dataset üõ†Ô∏è\n\nFirst, we import the required libraries which we will use to perform the current analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(naniar)\nlibrary(bookdown)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(forcats)\nlibrary(ggthemes)\nlibrary(corrplot)\nlibrary(mltools)\nlibrary(data.table)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(cowplot)\nlibrary(caTools)\nlibrary(pscl)\nlibrary(ROCR)\nlibrary(caret)\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(lightgbm)\nlibrary(Matrix)\nlibrary(catboost)\nlibrary(magrittr)\nlibrary(fmsb)\nlibrary(gbm)\n```\n:::\n\n\n\nGreat ! We have all the libraries loaded. Next, we are gonna load the required dataset for conducting the enzyme classification analysis. \n\nWe will use one dataset for the purpose of exploratory data analysis and training the classification model while the test dataset for testing the classification model on a completely new dataset.\n\n\nAfter reading the data, let us see how the train dataset looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- read_csv(\"data/train.csv\")\ndf_test <-  read_csv(\"data/test.csv\")\nhead(df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 38\n     id BertzCT  Chi1 Chi1n Chi1v Chi2n Chi2v Chi3v Chi4n EState_VSA1\n  <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>       <dbl>\n1     0    323.  9.88  5.88  5.88  4.30  4.30  2.75 1.75          0  \n2     1    274.  7.26  4.44  5.83  3.29  4.49  2.20 1.29         45.1\n3     2    522. 10.9   8.53 11.1   6.67  9.52  5.82 1.77         15.6\n4     3    567. 12.5   7.09 12.8   6.48 11.0   7.91 3.07         95.6\n5     4    113.  4.41  2.87  2.87  1.88  1.88  1.04 0.728        18.0\n6     5    145.  5.54  3.48  3.48  2.48  2.48  1.51 0.671        36.9\n# ‚Ñπ 28 more variables: EState_VSA2 <dbl>, ExactMolWt <dbl>,\n#   FpDensityMorgan1 <dbl>, FpDensityMorgan2 <dbl>, FpDensityMorgan3 <dbl>,\n#   HallKierAlpha <dbl>, HeavyAtomMolWt <dbl>, Kappa3 <dbl>,\n#   MaxAbsEStateIndex <dbl>, MinEStateIndex <dbl>, NumHeteroatoms <dbl>,\n#   PEOE_VSA10 <dbl>, PEOE_VSA14 <dbl>, PEOE_VSA6 <dbl>, PEOE_VSA7 <dbl>,\n#   PEOE_VSA8 <dbl>, SMR_VSA10 <dbl>, SMR_VSA5 <dbl>, SlogP_VSA3 <dbl>,\n#   VSA_EState9 <dbl>, fr_COO <dbl>, fr_COO2 <dbl>, EC1 <dbl>, EC2 <dbl>, ‚Ä¶\n```\n:::\n:::\n\n\nWe can observe that there are multiple process parameters present in the dataset which can help us analyse and predict the values of __EC1 and EC2__. But what do all these variables tell us ? Based on the information from [Panmie's Kaggle notebook](https://www.kaggle.com/code/tumpanjawat/s3e18-eda-cluster-ensemble-ada-cat-gb/notebook), following are the explanations of each of these variables.\n\n1. __Id__: This feature represents the identifier or unique identification number of a molecule. It serves as a reference but doesn't directly contribute to the predictive model.\n\n2. __BertzCT__: This feature corresponds to the Bertz complexity index, which measures the structural complexity of a molecule. It can provide insights into the intricacy of molecular structures.\n\n3. __Chi1__ : The Chi1 feature denotes the 1st order molecular connectivity index, which describes the topological connectivity of atoms in a molecule. It characterizes the atomic bonding pattern within the molecule.\n\n4. __Chi1n__ : This feature is the normalized version of the Chi1 index. It allows for standardized comparisons of the 1st order molecular connectivity across different molecules.\n\n5. __Chi1v__ : The Chi1v feature represents the 1st order molecular variance connectivity index. It captures the variance or diversity in the connectivity of atoms within a molecule.\n\n6. __Chi2n__ : The Chi2n feature indicates the 2nd order molecular connectivity index, which provides information about the extended connectivity of atoms in a molecule. It considers the neighboring atoms of each atom in the molecule.\n\n7. __Chi2v__ : Similar to Chi2n, the Chi2v feature measures the variance or diversity in the extended connectivity of atoms within a molecule at the 2nd order level.\n\n8. __Chi3v__ : The Chi3v feature represents the 3rd order molecular variance connectivity index. It captures the variance in the 3rd order connectivity patterns among atoms in a molecule.\n\n9. __Chi4n__ : This feature corresponds to the 4th order molecular connectivity index, which provides information about the extended connectivity of atoms in a molecule. The Chi4n index is normalized to allow for consistent comparisons across molecules.\n\n10. __EState_VSA1__ : EState_VSA1 is a feature that relates to the electrotopological state of a molecule. Specifically, it represents the Van der Waals surface area contribution for a specific atom type, contributing to the overall electrotopological state.\n\n11. __EState_VSA2__ : Similar to EState_VSA1, EState_VSA2 also represents the electrotopological state but for a different specific atom type.\n\n12. __ExactMolWt__ : This feature denotes the exact molecular weight of a molecule. It provides an accurate measurement of the mass of the molecule.\n\n13. __FpDensityMorgan1__ : FpDensityMorgan1 represents the Morgan fingerprint density for a specific radius of 1. Morgan fingerprints are a method for generating molecular fingerprints, and this feature captures the density of those fingerprints.\n\n14. __FpDensityMorgan2__ : Similar to FpDensityMorgan1, this feature represents the Morgan fingerprint density for a specific radius of 2.\n\n15. __FpDensityMorgan3__ : FpDensityMorgan3 corresponds to the Morgan fingerprint density for a specific radius of 3.\n\n16. __HallkierAlpha__ : The HallkierAlpha feature denotes the Hall-Kier alpha value for a molecule. It is a measure of molecular shape and can provide insights into the overall structure of the molecule.\n\n17. __HeavyAtomMolWt__ : This feature represents the molecular weight of heavy atoms only, excluding hydrogen atoms. It focuses on the mass of non-hydrogen atoms within the molecule.\n\n18. __Kappa3__ : The Kappa3 feature corresponds to the Hall-Kier Kappa3 value, which is a molecular shape descriptor. It provides information about the shape and spatial arrangement of atoms within the molecule.\n\n19. __MaxAbsEStateIndex__ : This feature represents the maximum absolute value of the E-state index. The E-state index relates to the electronic properties of a molecule, and its maximum absolute value can indicate the presence of specific electronic characteristics.\n\n20. __MinEStateIndex__ : MinEStateIndex denotes the minimum value of the E-state index. It provides information about the lowest observed electronic property value within the molecule.\n\n21. __NumHeteroatoms__ : This feature indicates the number of heteroatoms present in a molecule. Heteroatoms are atoms other than carbon and hydrogen, such as oxygen, nitrogen, sulfur, etc. This feature provides insights into the diversity and composition of atoms within the molecule.\n\n22. __PEOE_VSA10__ : PEOE_VSA10 represents the partial equalization of orbital electronegativity Van der Waals surface area contribution for a specific atom type. It captures the surface area contribution of a particular atom type to the overall electrostatic properties.\n\n23. __PEOE_VSA14__ : Similar to PEOE_VSA10, PEOE_VSA14 also represents the partial equalization of orbital electronegativity Van der Waals surface area contribution for a specific atom type.\n\n24. __PEOE_VSA6__ : This feature corresponds to the partial equalization of orbital electronegativity Van der Waals surface area contribution for a specific atom type at a different level.\n\n25. __PEOE_VSA7__ : Similar to PEOE_VSA6, PEOE_VSA7 represents the partial equalization of orbital electronegativity Van der Waals surface area contribution for a specific atom type.\n\n26. __PEOE_VSA8__ : PEOE_VSA8 denotes the partial equalization of orbital electronegativity Van der Waals surface area contribution for a specific atom type.\n\n27. __SMR_VSA10__ : SMR_VSA10 represents the solvent-accessible surface area Van der Waals surface area contribution for a specific atom type. It captures the contribution of a specific atom type to the solvent-accessible surface area.\n\n28. __SMR_VSA5__ : Similar to SMR_VSA10, this feature denotes the solvent-accessible surface area Van der Waals surface area contribution for a specific atom type at a different level.\n\n29. __SlogP_VSA3__ : The SlogP_VSA3 feature represents the LogP-based surface area contribution. It captures the contribution of a specific atom type to the surface area based on its logarithmic partition coefficient.\n\n30. __VSA_EState9__ : This feature denotes the E-state fragment contribution for the Van der Waals surface area calculation. It captures the fragment-specific contribution to the electrostatic properties of the molecule.\n\n31. __fr_COO__ : The fr_COO feature represents the number of carboxyl (COO) functional groups present in the molecule. It ranges from 0 to 8, providing insights into the presence and abundance of carboxyl groups.\n\n32. __fr_COO2__ : Similar to fr_COO, fr_COO2 represents the number of carboxyl (COO) functional groups, ranging from 0 to 8.\n\n33. __EC1__ : EC1 is a binary feature representing a predicted label related to __Oxidoreductases__. It serves as one of the target variables for prediction.\n\n34. __EC2__ : EC2 is another binary feature representing a predicted label related to __Transferases__. It serves as another target variable for prediction.\n\n\n# Data cleaning\n\n## Removal of unnecessary variables\n\nIn the first section, we will try to remove all the variables that will not be required for our analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- df_train %>% select(-c(\"id\",\"EC3\",\"EC4\",\"EC5\",\"EC6\"))\n```\n:::\n\n\n## Check for null values\n\nIn this step, we will try to check for the presence of null values in the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngg_miss_var(df_train)\n```\n\n::: {.cell-output-display}\n![Missingness in the dataset](index_files/figure-html/missvis-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the figure \\@ref(fig:missvis), we can observe that\n\n<div class=\"alert alert-block alert-success\">\n‚úÖ  The dataset does not contain any missing values. This indicates that we have a clean dataset which is ready for EDA and further analysis.\n</div>\n\n\n\n# Exploratory Data Analysis\n\nWe can observe that there are a total of __32 variables__ in the current dataset !!! <code style=\"background:yellow;color:red\">These are a lot more than what we would ideally like to analyse.</code> Such types of datasets require a special kind of analysis called as __High Dimensional Data Analysis__ which concentrate majorly on techniques such as clustering and pricipal component analysis to reduce the number of variables without completely losing data. While this is the right way to go about it, this notebook will however study the correlation of each variable and try to reduce the number of variables which are observed to show high multi-collinearity.\n\n\n## Correlation plot\n\n\nLet us understand how each of these variables correlate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrplot(cor(df_train),        # Correlation matrix\n         method = \"number\", # Correlation plot method\n         type = \"full\",    # Correlation plot style (also \"upper\" and \"lower\")\n         diag = TRUE,      # If TRUE (default), adds the diagonal\n         tl.col = \"black\", # Labels color\n         bg = \"white\",     # Background color\n         title = \"Correlation plot\",       # Main title\n         col = NULL,\n         number.cex = 0.4,\n         tl.cex = .5)\n```\n\n::: {.cell-output-display}\n![Correlation plot](index_files/figure-html/corrplot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAs we can observe from figure \\@ref(fig:corrplot),\n\n<div class=\"alert alert-block alert-info\">\nNone of the variables have an <strong> unusually high correlation with EC1 or EC2 </strong>. However, we do observe multiple variables which have high correlation to each other. This pheonmenon is called <strong> multi-collinearity </strong>. Let us set a correlation threshold of 75%. Any variables with correlation values higher than this will be dropped from the dataset.\n</div>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_corr = cor(df_train)\nhc = findCorrelation(df_corr, cutoff=0.75) # Removing variables with greater than 75% correlation\nhc = sort(hc)\ndf_train_new = df_train[,-c(hc)]\n```\n:::\n\n\nNow that we have removed the variables that observed to show multi-collinearity, let us now see how the revised dataset looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df_train_new) %>%\nDT::datatable(width = 500, height = 500, options = list(pageLength = 6))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-9e8c8d06151b36c6174f\" style=\"width:500px;height:500px;\" class=\"datatables html-widget \"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-9e8c8d06151b36c6174f\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[11.93829424,0,6.606881965,0,12.84164325,0],[1.181818182,1.346153846,1.085714286,1.162790698,1.444444444,1.083333333],[-0.24,-0.09,-0.78,-1.3,-1.1,-0.98],[8.17,3.201490511,15.03388953,6.724301321,3.931271989,3.015404674],[11.92250394,10.93233796,11.2380483,11.17117021,9.855740741,10.08746929],[0.171585412,-4.830449539,-5.066255336,-5.276575465,-1.676296296,-1.669305556],[0,24.41586555,0,42.72776471,6.041840829,0],[91.53649165,7.822697123,15.64539425,21.33513764,11.93861058,5.969305288],[0,0,0,0,6.9237372,0],[0,0,53.37823529,0,19.38639965,0],[0,0,0,6.420821623,0,0],[17.74406608,7.822697123,15.64539425,15.64539425,11.93861058,11.75255023],[0,30.70589228,73.14361574,62.10730426,18.88348408,24.41586555],[0,0,0,0,2,1],[1,0,1,1,1,1],[1,1,1,1,0,0]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>EState_VSA2<\\/th>\\n      <th>FpDensityMorgan1<\\/th>\\n      <th>HallKierAlpha<\\/th>\\n      <th>Kappa3<\\/th>\\n      <th>MaxAbsEStateIndex<\\/th>\\n      <th>MinEStateIndex<\\/th>\\n      <th>PEOE_VSA10<\\/th>\\n      <th>PEOE_VSA14<\\/th>\\n      <th>PEOE_VSA6<\\/th>\\n      <th>PEOE_VSA7<\\/th>\\n      <th>PEOE_VSA8<\\/th>\\n      <th>SMR_VSA10<\\/th>\\n      <th>SMR_VSA5<\\/th>\\n      <th>fr_COO2<\\/th>\\n      <th>EC1<\\/th>\\n      <th>EC2<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":6,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[6,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nLet us now create the correlation plot of the revised dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrplot(cor(df_train_new),        # Correlation matrix\n         method = \"number\", # Correlation plot method\n         type = \"full\",    # Correlation plot style (also \"upper\" and \"lower\")\n         diag = TRUE,      # If TRUE (default), adds the diagonal\n         tl.col = \"black\", # Labels color\n         bg = \"white\",     # Background color\n         title = \" \",       # Main title\n         col = NULL,\n         number.cex = 0.6,\n         tl.cex = .5)\n```\n\n::: {.cell-output-display}\n![Correlation plot of revised dataset](index_files/figure-html/corrplotnew-1.png){fig-align='center' width=672}\n:::\n:::\n\nFigure \\@ref(fig:corrplotnew) depicts the correlation values of all the variables which do not observe to demonstrate multi-collinearity.\n\n## Univariate analysis {#label1}\n\nNow that we have figured out the variables of interest, we will perform a univariate analysis of the revised dataset. One of the best ways to study the overall distribution of the variables is through a faceted histogram. Let us dive deeper.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl1 <- ggplot(data = gather(df_train_new), aes(x = value,fill = factor(key))) + geom_histogram() + facet_wrap(~key,scales =\"free_x\") + theme_classic() + ggtitle(\"Univariate analysis of variables\") + theme(legend.position = \"none\",plot.title = element_text(hjust = 0.5)) + labs(y=\"Number of instances\",x = \"Value of variable\")\npl1\n```\n\n::: {.cell-output-display}\n![Univariate analysis of variables](index_files/figure-html/facethist-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on our analysis of figure \\@ref(fig:facethist), we can observe that \n\n<div class=\"alert alert-block alert-warning\">\nwhile most variables range over a large scale in the X-axis, certain variables, namely <b> FpDensityMorgan1 </b> and <b> Kappa3 </b> range over a very small scale on the X-axis. This indicates that there is a large scale difference among the various variables. Hence, the dataset could benefit from <b>standardisation technique</b> at a later point of the analysis.\n</div>\n\n## Multi-variate analysis\n\nNow that we have performed a univariate analysis, it is now time to perform a multi-variate analysis to understand our variables better.\n\n## FpDensityMorgan1 and Kappa3\n\nLet us observe how do these variables differ for each values of EC.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl2 <- ggplot(data = df_train_new %>% filter(FpDensityMorgan1 > -5), aes(x =FpDensityMorgan1, y=Kappa3)) + geom_point(aes(color = factor(EC1))) + theme_classic() + labs(color = \"EC1 indicator\")\npl2\n```\n\n::: {.cell-output-display}\n![Kappa3 Vs FpDensityMorgan1 for EC1](index_files/figure-html/KappVMorgEC1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl2 <- ggplot(data = df_train_new %>% filter(FpDensityMorgan1 > -5), aes(x =FpDensityMorgan1, y=Kappa3)) + geom_point(aes(color = factor(EC2))) + theme_classic() + labs(color = \"EC2 indicator\")\npl2\n```\n\n::: {.cell-output-display}\n![Kappa3 Vs FpDensityMorgan1 for EC1](index_files/figure-html/KappVMorgEC2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the analysis from figures \\@ref(fig:KappVMorgEC1) and \\@ref(fig:KappVMorgEC2), we can observe that \n\n<div class=\"alert alert-block alert-warning\">\nüí°Most values of Kappa3 lie around 0 for both EC1 and EC2. There are very few datapoints which are higher than 0. Hence, majority of the datapoints can be clustered into one group.üí°\n</div>\n\n## EState_VSA2 and PEOE_VSA8\n\nThese variables were observed to have a fair amount of correlation to each other. Let us try to visualise these parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl3 <- ggplot(data = df_train_new, aes(x = EState_VSA2, y= PEOE_VSA8,color = factor(EC1))) + geom_point() + theme_classic() + labs(color = \"EC1 indicator\")\npl3\n```\n\n::: {.cell-output-display}\n![EState_VSA2 Vs PEOE_VSA8 for EC1](index_files/figure-html/EStateVSA8EC1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl4 <- ggplot(data = df_train_new, aes(x = EState_VSA2, y= PEOE_VSA8,color = factor(EC2))) + geom_point() + theme_classic() + labs(color = \"EC2 indicator\")\npl4\n```\n\n::: {.cell-output-display}\n![EState_VSA2 Vs PEOE_VSA8 for EC2](index_files/figure-html/EStateVSA8EC2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on figures \\@ref(fig:EStateVSA8EC1) and \\@ref(fig:EStateVSA8EC2), we can observe that\n\n<div class=\"alert alert-block alert-warning\">\nüí°there is no strong relationship for the two variables in each of the indicators.üí°\n</div>\n\nWhile we have reduced the number of variables for analysis, we still have a sizable number of variables to analyse. Let us instead plot the variable importance and choose the top 5 variables to study.\n\n\n## Feature importance\n\nTo study feature importances, let us use the XGBoost algorithm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\ndf_train_new_EC1 <- df_train_new %>% select(-EC2)\ndf_train_new_EC2 <- df_train_new %>% select(-EC1)\n\nsample_EC1=sample.split(df_train_new_EC1$EC1,SplitRatio=0.7)\ntrain_EC1=subset(df_train_new_EC1,sample_EC1==T)\ntest_EC1=subset(df_train_new_EC1,sample_EC1==F)\n\nsample_EC2=sample.split(df_train_new_EC2$EC2,SplitRatio=0.7)\ntrain_EC2=subset(df_train_new_EC2,sample_EC2==T)\ntest_EC2=subset(df_train_new_EC2,sample_EC2==F)\n```\n:::\n\n\n\n<div class=\"alert alert-block alert-success\">\nü•≥ Through the above code-chunk, we have successfully created separate train and test datasets for both the EC1 and EC2 indicators. The dataset is now ready for further processing.\n</div>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_model_EC1 <- xgboost(data = as.matrix(train_EC1 %>% select(-EC1)), label = as.matrix(train_EC1$EC1), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-logloss:0.604235 \n[2]\ttrain-logloss:0.592298 \n```\n:::\n\n```{.r .cell-code}\nxgb_model_EC2 <- xgboost(data = as.matrix(train_EC2 %>% select(-EC2)), label = as.matrix(train_EC2$EC2), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-logloss:0.499218 \n[2]\ttrain-logloss:0.494251 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_EC1_importance <- xgb.importance(colnames(train_EC1 %>% select(-EC1)), model = xgb_model_EC1, \n               data = as.matrix(train_EC1 %>% select(-EC1)), label = train_EC1$EC1)\n\nxgb_EC2_importance <- xgb.importance(colnames(train_EC2 %>% select(-EC2)), model = xgb_model_EC2, \n               data = as.matrix(train_EC2 %>% select(-EC2)), label = train_EC2$EC2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl5 <- ggplot(data =xgb_EC1_importance,aes(x = reorder(Feature,-Gain),y = round(Gain,2),fill=Feature)) + geom_col(color='black') + geom_label(aes(label = round(Gain,2))) + theme_classic() + ggtitle(\"Top 5 feature importances for EC1 \\n using XGBoost\") + labs(x = \"Feature name\",y = \"Feature importance\") + theme(plot.title = element_text(hjust = 0.5),legend.position = 'none',axis.title = element_text(face = 'bold'))\npl5\n```\n\n::: {.cell-output-display}\n![Feature importances for EC1 \n using XGBoost](index_files/figure-html/featplotEC1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl6 <- ggplot(data =xgb_EC2_importance,aes(x = reorder(Feature,-Gain),y = round(Gain,2),fill=Feature)) + geom_col(color='black') + geom_label(aes(label = round(Gain,2))) + theme_classic() + ggtitle(\"Top 5 feature importances for EC2 \\n using XGBoost\") + labs(x = \"Feature name\",y = \"Feature importance\") + theme(plot.title = element_text(hjust = 0.5),legend.position = 'none',axis.title = element_text(face = 'bold')) + scale_fill_brewer(palette = 'Accent')\npl6\n```\n\n::: {.cell-output-display}\n![Feature importances for EC2 \n using XGBoost](index_files/figure-html/featplotEC2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on figures \\@ref(fig:featplotEC1) and \\@ref(fig:featplotEC2), \n\n<div class=\"alert alert-block alert-warning\">\nüí° we have obtained the top 5 features using the XGBoost model for each of the EC1 and EC2 indicators. This will help us concentrate our EDA efforts only on the most important features.üí°\n</div>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train_new_EC1_scaled <- as.data.frame(df_train_new_EC1 %>% filter(EC1 == 1) %>% scale(center=TRUE,scale=TRUE))\nEC1_imp <- as.data.frame(df_train_new_EC1_scaled %>% select(-EC1) %>%  summarise_all(median,na.rm=TRUE))\n\nEC1_imp <- rbind(rep(1,ncol(df_train_new_EC1_scaled -1)) , rep(-1,ncol(df_train_new_EC1_scaled -1)),EC1_imp)\n\n\ndf_train_new_EC2_scaled <- as.data.frame(df_train_new_EC2 %>% filter(EC2 == 1) %>% scale(center=TRUE,scale=TRUE))\nEC2_imp <- as.data.frame(df_train_new_EC2_scaled %>% select(-EC2) %>%  summarise_all(median,na.rm=TRUE))\n\nEC2_imp <- rbind(rep(1,ncol(df_train_new_EC2_scaled -1)) , rep(-1,ncol(df_train_new_EC2_scaled -1)),EC2_imp)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nEC_imp <- EC1_imp\nEC_imp <- rbind(EC1_imp,EC2_imp[3,])\nrownames(EC_imp) <- 1:nrow(EC_imp)\n\n\nareas <- c(rgb(1, 0, 0, 0.3),\n           rgb(0, 1, 0, 0.3))\n\nradarchart(EC_imp,\n           axistype = 1,\n           cglcol=\"gray\", cglty=1, axislabcol=\"gray\", caxislabels=seq(-1,1,0.5), cglwd=0.8,\n           pcol = 2:4,      # Color for each line\n           plwd = 4,        # Width for each line\n           plty = 1,        # Line type for each line\n           pfcol = areas,   # Color of the areas\n           vlcex=0.5,      # Size of label\n           title=paste(\"Median standaridised values for EC1 and EC2 indicators\")\n           \n           )   \n\nlegend(\"topright\",\n       legend = paste(c(\"EC1\",\"EC2\")),\n       bty = \"n\", pch = 20, col = areas,\n       text.col = \"grey25\", pt.cex = 2)\n```\n\n::: {.cell-output-display}\n![Median standaridised values for EC1 and EC2 indicators](index_files/figure-html/radplot-1.png){fig-align='center' width=672}\n:::\n:::\n\nAfter analysing \\@ref(fig:radplot), we observe that\n\n<div class=\"alert alert-block alert-warning\">\nüí° The median standardised values for both EC1 and EC2 indicators are very similar. The major difference however can be observed for the variable PEOE_VSA7. EC2 is observed to demonstrate a relatively lower scaled value when compared to EC1.üí°\n</div>\n\n\n# Classification model\n\n## Feature Engineering\n\nAs explained in \\@ref(label1), the dataset may benefit through a standardisation process. Let us standardise the variables to the same values as observed in figure \\@ref(fig:radplot).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train_new_EC1_scaled <- as.data.frame(df_train_new_EC1[,1:ncol(df_train_new_EC1) - 1] %>% scale(center = TRUE,scale = TRUE))\ndf_train_new_EC1_scaled <- cbind(df_train_new_EC1_scaled,EC1 = df_train_new_EC1$EC1)\ndf_train_new_EC2_scaled <- as.data.frame(df_train_new_EC2[,1:ncol(df_train_new_EC2) - 1] %>% scale(center = TRUE,scale = TRUE))\ndf_train_new_EC2_scaled <- cbind(df_train_new_EC2_scaled,EC2 = df_train_new_EC2$EC2)\n```\n:::\n\n\n\n## Creating train and test dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\nsample_EC1=sample.split(df_train_new_EC1_scaled$EC1,SplitRatio=0.7)\ntrain_EC1=subset(df_train_new_EC1_scaled,sample_EC1==T)\ntest_EC1=subset(df_train_new_EC1_scaled,sample_EC1==F)\n\nsample_EC2=sample.split(df_train_new_EC2_scaled$EC2,SplitRatio=0.7)\ntrain_EC2=subset(df_train_new_EC2_scaled,sample_EC2==T)\ntest_EC2=subset(df_train_new_EC2_scaled,sample_EC2==F)\n```\n:::\n\n\n\n## Logistic Regression\n\nWe will first apply the logistic regression algorithm to perform classification for EC1 and EC2 indicators.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_logit_EC1 <- glm(EC1~.,family=binomial(link='logit'),data=train_EC1)\npR2(model_logit_EC1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfitting null model for pseudo-r2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n          llh       llhNull            G2      McFadden          r2ML \n-6.179552e+03 -6.603589e+03  8.480734e+02  6.421306e-02  7.840331e-02 \n         r2CU \n 1.089552e-01 \n```\n:::\n\n```{.r .cell-code}\nmodel_logit_EC2 <- glm(EC2~.,family=binomial(link='logit'),data=train_EC2)\npR2(model_logit_EC2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfitting null model for pseudo-r2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n          llh       llhNull            G2      McFadden          r2ML \n-5.165338e+03 -5.212117e+03  9.355681e+01  8.974935e-03  8.967523e-03 \n         r2CU \n 1.415614e-02 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.resultsEC1 <- predict(model_logit_EC1,newdata=subset(test_EC1,select=-(EC1)),type='response')\nfitted.resultsEC1 <- ifelse(fitted.resultsEC1 > 0.5,1,0)\n\nmisClasificError <- mean(fitted.resultsEC1 != test_EC1$EC1)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy of logistic regression: 0.690406650190968\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.resultsEC2 <- predict(model_logit_EC2,newdata=subset(test_EC2,select=-(EC2)),type='response')\nfitted.resultsEC2 <- ifelse(fitted.resultsEC2 > 0.5,1,0)\n\nmisClasificError <- mean(fitted.resultsEC2 != test_EC2$EC2)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy of logistic regression: 0.79874213836478\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_confusion_matrix <- function(cm) {\n\n  layout(matrix(c(1,1,2)))\n  par(mar=c(2,2,2,2))\n  plot(c(100, 345), c(300, 450), type = \"n\", xlab=\"\", ylab=\"\", xaxt='n', yaxt='n')\n  title('CONFUSION MATRIX', cex.main=2)\n\n  # create the matrix \n  rect(150, 430, 240, 370, col='#3F97D0')\n  text(195, 435, 'False', cex=1.2)\n  rect(250, 430, 340, 370, col='#F7AD50')\n  text(295, 435, 'True', cex=1.2)\n  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)\n  text(245, 450, 'Actual', cex=1.3, font=2)\n  rect(150, 305, 240, 365, col='#F7AD50')\n  rect(250, 305, 340, 365, col='#3F97D0')\n  text(140, 400, 'False', cex=1.2, srt=90)\n  text(140, 335, 'True', cex=1.2, srt=90)\n\n  # add in the cm results \n  res <- as.numeric(cm$table)\n  text(195, 400, res[1], cex=1.6, font=2, col='white')\n  text(195, 335, res[2], cex=1.6, font=2, col='white')\n  text(295, 400, res[3], cex=1.6, font=2, col='white')\n  text(295, 335, res[4], cex=1.6, font=2, col='white')\n\n  # add in the specifics \n  plot(c(100, 0), c(100, 0), type = \"n\", xlab=\"\", ylab=\"\", main = \"DETAILS\", xaxt='n', yaxt='n')\n  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)\n  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)\n  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)\n  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)\n  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)\n  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)\n  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)\n  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)\n  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)\n  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)\n\n  # add in the accuracy information \n  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)\n  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)\n  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)\n  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)\n}  \n```\n:::\n\n\nAs we can observe, \n\n<div class=\"alert alert-block alert-warning\">\nüí° the logistic regression model was able to <b> accurately predict approximately 70% of the cases with EC1 and EC2 indicators </b>.üí°\n</div>\n\nLet us further study the performance of the logistic regression model through the __Receiver Operating Curve (ROC) metric__.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- as.numeric(predict(model_logit_EC1, newdata=subset(test_EC1,select=-c(EC1)), type=\"response\"))\nq <- as.numeric(predict(model_logit_EC2, newdata=subset(test_EC2,select=-c(EC2)), type=\"response\"))\npr <- prediction(p, test_EC1$EC1)\npo <- prediction(q,test_EC2$EC2)\nprf <- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\n\nprf2 <- performance(po, measure = \"tpr\", x.measure = \"fpr\")\nplot( prf, col = 'blue')\nplot(prf2, add = TRUE, col = 'red')\n\nlegend(\"right\", c(\"EC1\", \"EC2\"), lty=1, \n    col = c(\"blue\", \"red\"), bty=\"n\", inset=c(0,-0.15))\n\ntitle(\"Receiver Operating Curve for Logistic Regression\")\n```\n\n::: {.cell-output-display}\n![Receiver Operating Curve for Logistic Regression](index_files/figure-html/roc-logit-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the analysis of the ROC plot in figure \\@ref(fig:roc-logit), we observe that,\n\n<div class=\"alert alert-block alert-warning\">\nüí° the logistic model is able to predict the cases for EC1 better than EC2 due to the curve covering higher area under the true positivity rate. </b>üí°\n</div>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_logit_EC1 <- confusionMatrix(factor(fitted.resultsEC1),factor(test_EC1$EC1))\ndraw_confusion_matrix(cm_logit_EC1)\n```\n\n::: {.cell-output-display}\n![Confusion matrix for Logistic Regression EC1](index_files/figure-html/conf-matEC1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_logit_EC2 <- confusionMatrix(factor(fitted.resultsEC2),factor(test_EC2$EC2))\ndraw_confusion_matrix(cm_logit_EC2)\n```\n\n::: {.cell-output-display}\n![Confusion matrix for Logistic Regression EC2](index_files/figure-html/conf-matEC2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## XGboost\n\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_model_EC1 <- xgboost(data = as.matrix(train_EC1 %>% select(-EC1)), label = as.matrix(train_EC1$EC1), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-logloss:0.604235 \n[2]\ttrain-logloss:0.592298 \n```\n:::\n\n```{.r .cell-code}\nxgb_model_EC2 <- xgboost(data = as.matrix(train_EC2 %>% select(-EC2)), label = as.matrix(train_EC2$EC2), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-logloss:0.499218 \n[2]\ttrain-logloss:0.494251 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_xgb_EC1 <- predict(xgb_model_EC1, as.matrix(test_EC1 %>% select(-c(EC1))))\npred_xgb_EC1 <- if_else(pred_xgb_EC1 > 0.5,1,0)\n\n\npred_xgb_EC2 <- predict(xgb_model_EC2, as.matrix(test_EC2 %>% select(-c(EC2))))\npred_xgb_EC2 <- if_else(pred_xgb_EC2 > 0.5,1,0)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_xgb_EC1 <- confusionMatrix(factor(pred_xgb_EC1),factor(test_EC1$EC1))\ndraw_confusion_matrix(cm_xgb_EC1)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the XGBoost model for EC1](index_files/figure-html/conf-mat-xgbEC1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_xgb_EC2 <- confusionMatrix(factor(pred_xgb_EC2),factor(test_EC2$EC2))\ndraw_confusion_matrix(cm_xgb_EC2)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the XGBoost model for EC2](index_files/figure-html/conf-mat-xgbEC2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Light GBM\n\nLet us see how does the LGBM  dataset perform on the current dataset.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_lgb_EC1 = predict(model_lgb_EC1, as.matrix(test_EC1))\npred_lgb_EC1 <- if_else(pred_lgb_EC1 > 0.5,1,0)\ncm_lgb_EC1 <- confusionMatrix(factor(pred_lgb_EC1),factor(test_EC1$EC1))\ndraw_confusion_matrix(cm_lgb_EC1)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Light GB model for EC1](index_files/figure-html/cf-lgb1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_lgb_EC2 = predict(model_lgb_EC2, as.matrix(test_EC2))\npred_lgb_EC2 <- if_else(pred_lgb_EC2 > 0.5,1,0)\ncm_lgb_EC2 <- confusionMatrix(factor(pred_lgb_EC2),factor(test_EC2$EC2))\ndraw_confusion_matrix(cm_lgb_EC2)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Light GB model for EC2](index_files/figure-html/cf-lgb2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nJudging from the confusion matrices in figures \\@ref(fig:cf-lgb1) and \\@ref(fig:cf-lgb2), \n\n<div class=\"alert alert-block alert-warning\">\nüí° The Light GBM model has been observed to lack precision for each of the indicators in this dataset.üí°\n</div>\n\n\n## Gradient Boosted Decision Trees\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngbm_EC1=gbm(EC1~ . ,data = train_EC1,distribution = \"gaussian\",n.trees = 10000,\n                  shrinkage = 0.01, interaction.depth = 4)\n\nsummary(gbm_EC1)\n```\n\n::: {.cell-output-display}\n![Feature importance with GBDT for EC1](index_files/figure-html/gbmec1-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                var   rel.inf\nMinEStateIndex       MinEStateIndex 16.102255\nMaxAbsEStateIndex MaxAbsEStateIndex 11.364042\nKappa3                       Kappa3 10.873453\nFpDensityMorgan1   FpDensityMorgan1  9.080561\nHallKierAlpha         HallKierAlpha  8.789347\nSMR_VSA5                   SMR_VSA5  7.591320\nPEOE_VSA14               PEOE_VSA14  6.811754\nSMR_VSA10                 SMR_VSA10  6.345157\nPEOE_VSA10               PEOE_VSA10  6.034478\nEState_VSA2             EState_VSA2  5.331302\nPEOE_VSA8                 PEOE_VSA8  4.013482\nPEOE_VSA7                 PEOE_VSA7  4.000617\nPEOE_VSA6                 PEOE_VSA6  2.630633\nfr_COO2                     fr_COO2  1.031598\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngbm_EC2=gbm(EC2~ .,data = train_EC2,distribution = \"gaussian\",n.trees = 10000,\n                  shrinkage = 0.01, interaction.depth = 4)\n\nsummary(gbm_EC2)\n```\n\n::: {.cell-output-display}\n![Feature importance with GBDT for EC2](index_files/figure-html/gbmec2-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                var   rel.inf\nMinEStateIndex       MinEStateIndex 12.764873\nMaxAbsEStateIndex MaxAbsEStateIndex 12.714630\nKappa3                       Kappa3 12.174385\nFpDensityMorgan1   FpDensityMorgan1 10.094792\nHallKierAlpha         HallKierAlpha  8.590496\nSMR_VSA5                   SMR_VSA5  7.871469\nEState_VSA2             EState_VSA2  6.237231\nSMR_VSA10                 SMR_VSA10  5.186050\nPEOE_VSA6                 PEOE_VSA6  4.952903\nPEOE_VSA7                 PEOE_VSA7  4.770957\nPEOE_VSA10               PEOE_VSA10  4.522524\nPEOE_VSA8                 PEOE_VSA8  4.277605\nPEOE_VSA14               PEOE_VSA14  3.745410\nfr_COO2                     fr_COO2  2.096675\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_gbm_EC1 = predict(gbm_EC1, test_EC1)\npred_gbm_EC1 <- if_else(pred_gbm_EC1 > 0.5,1,0)\ncm_gbm_EC1 <- confusionMatrix(factor(pred_gbm_EC1),factor(test_EC1$EC1))\ndraw_confusion_matrix(cm_gbm_EC1)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Light GB model for EC1](index_files/figure-html/cf-gbd1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_gbm_EC2 = predict(gbm_EC2, test_EC2)\npred_gbm_EC2 <- if_else(pred_gbm_EC2 > 0.5,1,0)\ncm_gbm_EC2 <- confusionMatrix(factor(pred_gbm_EC2),factor(test_EC2$EC2))\ndraw_confusion_matrix(cm_gbm_EC2)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Light GB model for EC1](index_files/figure-html/cf-gbd2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Random Forest\n\nLet us use an ensemble algorithm to classify our results. We shall utilise the Random Forest technique which utilises multiple decision trees to predict results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model_EC1<-randomForest(EC1~.,data=train_EC1)\nrf_model_EC2<-randomForest(EC2~.,data=train_EC2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(rf_model_EC1)\n```\n\n::: {.cell-output-display}\n![Error vs Number of trees for Random Forest for EC1](index_files/figure-html/errormod-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(rf_model_EC2)\n```\n\n::: {.cell-output-display}\n![Error vs Number of trees for Random Forest for EC1](index_files/figure-html/errormod1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_rf_EC1 <- predict(rf_model_EC1,test_EC1, type = 'class')\npred_rf_EC1 <- if_else(pred_rf_EC1 > 0.5,1,0)\ncm_rf <- confusionMatrix(factor(pred_rf_EC1),factor(test_EC1$EC1))\ndraw_confusion_matrix(cm_rf)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the random forest model for EC1](index_files/figure-html/conf-mat-rf1-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_rf_EC2 <- predict(rf_model_EC2,test_EC2, type = 'class')\npred_rf_EC2 <- if_else(pred_rf_EC2 > 0.5,1,0)\ncm_rf <- confusionMatrix(factor(pred_rf_EC2),factor(test_EC2$EC2))\ndraw_confusion_matrix(cm_rf)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the random forest model for EC2](index_files/figure-html/conf-mat-rf2-1.png){fig-align='center' width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<link href=\"../../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/datatables-binding-0.28/datatables.js\"></script>\n<script src=\"../../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../../site_libs/dt-core-1.13.4/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../../site_libs/dt-core-1.13.4/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/dt-core-1.13.4/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}