{
  "hash": "c6492bafa9aee9d14316495613dc656f",
  "result": {
    "markdown": "---\ntitle: \"üî©üõ†Ô∏è‚ö†Ô∏è Binary Machine Failure Prediction\"\nauthor: \"Arindam Baruah\"\ndate: \"2023-06-13\"\noutput:\n  bookdown::html_document2:\n   \n    toc: false\n    toc_float: true\n    theme: united\n---\n\n\n\n\n\n\n# Introduction\n\nBinary machine failure prediction using machine learning is a technique employed to anticipate the occurrence of failures or malfunctions in a binary system or machine. With the increasing complexity of modern machines, the ability to predict and prevent failures becomes crucial for optimizing performance, reducing downtime, and avoiding costly repairs.\n\nMachine learning algorithms play a vital role in this prediction process by analyzing historical data and identifying patterns or anomalies that indicate potential failures. These algorithms learn from past failure instances, considering various factors such as sensor readings, environmental conditions, maintenance records, and other relevant parameters.\n\nThe predictive models are trained on labeled datasets, where each instance is associated with a failure or non-failure outcome. Common machine learning techniques used for binary machine failure prediction include __logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks__.\n\nDuring the training phase, the algorithms learn the relationships between input features and failure occurrences, thereby enabling them to make accurate predictions on unseen data. Feature engineering, which involves selecting or transforming relevant input variables, is an essential step in improving the model's performance.\n\nOnce the model is trained, it can be deployed to make real-time predictions on new data streams. By continuously monitoring machine inputs and comparing them to the learned patterns, the system can generate alerts or take preventive actions whenever a potential failure is detected. This proactive approach helps minimize unexpected downtime, reduce maintenance costs, and improve overall operational efficiency.\n\nBinary machine failure prediction using machine learning is widely applied across various industries, including manufacturing, power generation, healthcare, transportation, and more. By leveraging the power of data and advanced analytics, it offers a valuable tool for optimizing maintenance strategies, enhancing productivity, and ensuring the reliability of critical systems.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknitr::include_graphics(\"mach_failure.jpeg\")\n```\n\n::: {.cell-output-display}\n![Source: www.gesrepair.com](mach_failure.jpeg){fig-align='center' width=512}\n:::\n:::\n\n\n# Importing the relevant libraries and dataset üõ†Ô∏è\n\nFirst, we import the required libraries which we will use to perform the current analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(naniar)\nlibrary(bookdown)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(forcats)\nlibrary(ggthemes)\nlibrary(corrplot)\nlibrary(mltools)\nlibrary(data.table)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(cowplot)\nlibrary(caTools)\nlibrary(pscl)\nlibrary(ROCR)\nlibrary(caret)\nlibrary(xgboost)\nlibrary(randomForest)\nlibrary(lightgbm)\nlibrary(Matrix)\nlibrary(catboost)\nlibrary(magrittr)\nlibrary(fmsb)\n```\n:::\n\n\nGreat ! We have all the libraries loaded. Next, we are gonna load the required dataset for conducting the machine failure classification analysis. \n\nWe will use one dataset for the purpose of exploratory data analysis and training the classification model while the test dataset for testing the classification model on a completely new dataset.\n\nAfter reading the data, let us see how the train dataset looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- read_csv(\"data/train.csv\")\ndf_test <-  read_csv(\"data/test.csv\")\nhead(df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 14\n     id `Product ID` Type  `Air temperature [K]` `Process temperature [K]`\n  <dbl> <chr>        <chr>                 <dbl>                     <dbl>\n1     0 L50096       L                      301.                      310.\n2     1 M20343       M                      303.                      312.\n3     2 L49454       L                      299.                      308.\n4     3 L53355       L                      301                       311.\n5     4 M24050       M                      298                       309 \n6     5 M24300       M                      298.                      309.\n# ‚Ñπ 9 more variables: `Rotational speed [rpm]` <dbl>, `Torque [Nm]` <dbl>,\n#   `Tool wear [min]` <dbl>, `Machine failure` <dbl>, TWF <dbl>, HDF <dbl>,\n#   PWF <dbl>, OSF <dbl>, RNF <dbl>\n```\n:::\n:::\n\n\nWe can observe that there are multiple process parameters present in the dataset which can help us analyse whether a machine undergoes failure. We can also observe that there are multiple abbreviations in this dataset. Let us try to understand what do these abbreviations mean :\n\n1. __Tool Wear Failure (TWF)__: A type of machine failure which is associated with excessive tool wear.\n2. __Heat Dissipation Failure (HDF)__: Machine failures which are associated with high process temperatures.\n3. __Power Failure (PWF)__: Machine failures which are associated with power readings above or below a certain value.\n4. __Overstrain Failure (OSF)__: Machine failures which are associated with high strain values.\n5. __Random Failure (RNF)__:  Machine failures which maybe associated with random conditions.\n\n# Data cleaning\n\n## Check for null values\n\nAs a part of checking for the cleanliness of the dataset, let us visaulise the presence of null values for each of the variables.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngg_miss_var(df_train)\n```\n\n::: {.cell-output-display}\n![Missingness in the dataset](Machine_failure_preds_files/figure-html/missvis-1.png){fig-align='center' width=672}\n:::\n:::\n\nAs we can observe from figure \\@ref(fig:missvis), there are no missing values for any of the variables in the dataset. As a result, the __dataset can be considered clean__ for further analysis.\n\n## Removal of variables\n\nAfter studying for the presence of null values, we now remove the variables that do not provide any extra insights into our analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- df_train %>% select(-c(id,`Product ID`))\n```\n:::\n\n\n## Cleaning the variable names\n\nThe current dataset contains variable names which are not ideal for data wrangling and EDA. Hence, we will try to remove any unnecessary white space and special characters for each of the variable names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- clean_names(df_train)\nhead(df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 12\n  type  air_temperature_k process_temperature_k rotational_speed_rpm torque_nm\n  <chr>             <dbl>                 <dbl>                <dbl>     <dbl>\n1 L                  301.                  310.                 1596      36.1\n2 M                  303.                  312.                 1759      29.1\n3 L                  299.                  308.                 1805      26.5\n4 L                  301                   311.                 1524      44.3\n5 M                  298                   309                  1641      35.4\n6 M                  298.                  309.                 1429      42.1\n# ‚Ñπ 7 more variables: tool_wear_min <dbl>, machine_failure <dbl>, twf <dbl>,\n#   hdf <dbl>, pwf <dbl>, osf <dbl>, rnf <dbl>\n```\n:::\n:::\n\n\n\n\n# Exploratory Data Analysis\n\nAfter obtaining the cleansed dataset, we now try to visualise the relationship of each of the variables and attempt to obtain critical insights.\n\n## Type of machine\n\nThere are a total of 3 types  machines in this dataset. These are encoded as:\n\n1. __L (Light)__\n2. __M (Medium)__\n3. __H (Heavy)__\n\nLet us see the number of machine failures for each of the machine types.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfacet_lookup <- c(\"H\" = \"Heavy (H)\",\"L\"= \"Light (L)\",\"M\" = \"Medium (M)\")\ndf_type_group <- df_train %>% group_by(type,machine_failure) %>% summarise(count = n())\ndf_type_group <- df_type_group %>% group_by(type) %>% mutate(total = sum(count))\ndf_type_group <- df_type_group %>% mutate(percentage = 100 * (count/total))\npl1 <- ggplot(data = df_type_group, \n              aes(x = factor(machine_failure),\n                  y = count)) + geom_col(aes(fill = type),color='black') + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup)) + geom_label(aes(label = count)) + labs(x = \"Machine failure status\", \n                                                                                                                       y = \"Number of incidents\") + ggtitle(\"Number of machine failures for each type\") + theme_classic() + theme(legend.position = 'none') \npl1\n```\n\n::: {.cell-output-display}\n![Number of machine failures for each type](Machine_failure_preds_files/figure-html/failtype-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nFigure \\@ref(fig:failtype) illustrates the number of failures observed for each machine type. The failures constitute:\\\n- __1 %__ of the incidents for machine type \"H\"\\\n- __2 %__ of the incidents for machine type \"L\"\\\n- __1 %__ of the incidents for machine type \"M\"\\\n\n<div class=\"alert alert-info\">\n  <strong> üìµ Hence, we can observe that the number of failure cases are <strong>fairly evenly distributed among each of the machine types.</strong> üìµ .\n</div>\n\n\n## Air and process temperatures {#label2}\n\nTemperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).\n\nLeet us first study the distribution of the temperature values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl2 <- ggplot(data = df_train,\n              aes(x = process_temperature_k,\n                  fill = factor(hdf)),\n              alpha = 0.6) + geom_histogram(alpha = 0.8,\n                                            position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Process temperature (K)\" , y = \"Number of incidences\", fill = 'HDF status')\n\npl3 <- ggplot(data = df_train,\n              aes(x = air_temperature_k,\n                  fill = factor(hdf))) + geom_histogram(alpha = 0.8,\n                                                        position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Air temperature (K)\" , y = \"Number of incidences\", fill = 'HDF status')\n\nplot_grid(pl3,pl2, labels = c(\"Air temperatures\",\"Process temperatures\"))\n```\n\n::: {.cell-output-display}\n![Temperature variation distribution](Machine_failure_preds_files/figure-html/temps-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAs we can observe from \\@ref(fig:temps), majority of the heat dissipation failures have occurred __at relatively higher values of air temperatures__. These air temperatures are observed to be __around 302.5 K__. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.\n\nHeat dissipation values are governed by the following heat transfer equation.\n\n$$ \\boxed{\\Delta H = mC_p(T_{process} - T_{air})}$$\nBased on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_temp_diff <- df_train %>% select(c(process_temperature_k,air_temperature_k,hdf)) %>% mutate(temp_diff = process_temperature_k - air_temperature_k)\n\n\npl4 <- ggplot(data = df_temp_diff,\n              aes(x = temp_diff,\n                  fill = factor(hdf)),\n              alpha = 0.6) + geom_histogram(alpha = 0.8,\n                                            position = 'identity',color='black') + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Temperature difference (K)\" , y = \"Number of incidences\", fill = 'HDF status') + ggtitle(\"Heat dissipation failure based on temperature difference\") + \n  \n  annotate(\"segment\",x = 5,\n    y = 2500,xend = 7 ,\n    yend = 5 ,arrow = arrow(type = \"closed\", \n                              length = unit(0.02, \"npc\"))\n  ) +\n  annotate(\"text\",x = 5,\n    y = 3800,colour = \"red\",\n    label = 'High chances of HDF \\n due to low temperature difference',\n    size = unit(3, \"pt\")) + theme(axis.text.x = element_text(angle = 10,face = 'bold')) \npl4\n```\n\n::: {.cell-output-display}\n![Heat dissipation failure based on temperature difference](Machine_failure_preds_files/figure-html/tempdiff-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nAs illustrated by figure \\@ref(fig:tempdiff) and based on the heat transfer equation, we can observe that \n\n\n<div class=\"alert alert-info\">\n  <strong> ‚ö†Ô∏è the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures. ‚ö†Ô∏è </strong> .\n</div>\n\n## Torque and Tool rotation speed {#label1}\n\nThe torque of a machine can be defined as the amount of rotational energy required to perform mechanical work. As a result of torque applied, a machine element, such as the tool in this case rotates at a particular speed. This speed of rotation is measured by the tool rotation speed in revolutions per minute (RPM).\n\nIn addition to the above definitions, the product of the torque and the tool rotation speeds give us the value of the power consumption of a machine. The equaiton for the same is as follows :\n\n$$ \\boxed{P = \\omega T} $$\nWhere,\n\n$P =$ Power consumption of the machine in Watts\\\n$T =$ Torque in Nm\\\n$\\omega =  2\\pi N/60$, with $N$ being the tool rotational speed (RPM)\\\n\nLet us now visualise the torque and tool rotation speed values for each machine type.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl5 <- ggplot(data = df_train, aes(x = torque_nm , y = rotational_speed_rpm)) + geom_bin_2d() + theme_classic() + labs(x = \"Torque (Nm)\", \n                                                                                                                                      y = \"Rotational speed (RPM)\") +  ggtitle(\"Working window of Torque and Rotational speeds\")\npl5\n```\n\n::: {.cell-output-display}\n![Working window of Torque and Rotational speeds](Machine_failure_preds_files/figure-html/pwf-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the plot in figure \\@ref(fig:pwf), we can observe \n\n<div class=\"alert alert-info\">\n  <strong> ‚ö†Ô∏èThe ideal working window for torque lies between 25 Nm - 50 Nm while that for the rotational speed lies between 1250-2000 RPM . ‚ö†Ô∏è </strong> .\n</div>\n\nLet us now try to study how do the values for tool rotation speeds and torque vary based on power failure (PWF) faceted for each machine type.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl6 <- ggplot(data = df_train, aes(x = torque_nm , \n                                   y = rotational_speed_rpm, \n                                   fill = factor(pwf))) + geom_hex() + scale_fill_manual(values = c(\"blue\", \"red\")) + theme_classic() + labs(x = \"Torque (Nm)\", \n                                                                                               y = \"Rotational speed (RPM)\") + guides(color = FALSE) + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup)) + labs(fill = \"PWF status\") + ggtitle(\"Torque and rotational speed window \\n for each machine type\") + theme(plot.title = element_text(hjust=0.5))\npl6\n```\n\n::: {.cell-output-display}\n![Faceted hex plot of working windows for each machine type](Machine_failure_preds_files/figure-html/facetwindown-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the analysis of figure \\@ref(fig:facetwindown) we can conclude that\n\n<div class=\"alert alert-info\">\n  ‚ùå Power failures (PWF) are observed to majorly occur outside the ideal working window. These failures are <strong> majorly concentrated in either the regions of high torque and low rotational speeds or low torque and high rotational speeds. The observation holds consistent for all three machine types ‚ùå </strong> .\n</div>\n\n## Power consumption\n\nBased on the analysis in section \\@ref(label1), let us study how does the power consumption differ for machines which have undergone power failure (PWF).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_train <- df_train %>% mutate(power_w = torque_nm * 2* pi * rotational_speed_rpm/60)\n\npl7 <- ggplot(data = df_train,\n              aes(x = power_w ,\n                  fill = factor(pwf))) + geom_density(alpha = 0.6) + theme_classic() + scale_fill_manual(values = c(\"#3268a8\",\"#9c1144\")) + labs(x = \"Power consumption (in W)\",y = \"Probability density\",fill = \"PWF status\" ) + ggtitle(\"Probability density of power consumption for each PWF status\") + facet_wrap(~type,\n                                                                                      labeller = as_labeller(facet_lookup))\npl7\n```\n\n::: {.cell-output-display}\n![Probability density of power consumption for each PWF status](Machine_failure_preds_files/figure-html/power-1.png){fig-align='center' width=672}\n:::\n:::\n\nFigure \\@ref(fig:power) illustrates the density distribution of power consumption for each power failure (PWF) status and faceted for each of the machine types. We can observe that:\n\n<div class=\"alert alert-info\">\n  ‚ö°Ô∏èthe density plot for machines which have undergone power failure is bimodal in nature while the plot is unimodal for machines which did not undergo power failure. Based on the density plot, we can observe that the ideal working window for power consumption should be between 4000-10000 W. Machines reporting power consumption below or above this band are observed to be prone to undergo power failure.‚ö°Ô∏è\n</div>\n\n## Toolwear \n\nThe toolwear can play a critical role in terms of overstrain failure (OSF) as __it can lead to excess loads on various parts of the machine equipment.__ Hence, it is pertinent to study the importance of toolwear through visualisations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npl8 <- ggplot(data = df_train, aes(x= factor(twf),y = tool_wear_min, fill = factor(twf))) + geom_violin() + scale_fill_manual(values = c('blue','red')) + geom_boxplot(width=0.1, color=\"black\", alpha=0.2) + theme_classic() + labs(x = 'Overstrain failure (OSF) status',y = \"Tool wear (mm)\") + guides(fill = FALSE) + ggtitle(\"Tool wear values for overstrain failure status (OSF) \\n faceted for each machine type\") + facet_wrap(~type,labeller = as_labeller(facet_lookup))\npl8\n```\n\n::: {.cell-output-display}\n![Tool wear values for overstrain failure status (OSF) faceted for each machine type](Machine_failure_preds_files/figure-html/toolwear-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAfter analysing figure \\@ref(fig:toolwear), we observe that\n\n\n<div class=\"alert alert-info\">\n  üî© Overstrain failures as a result of tool wear occurs <strong>majorly for tool wear values of 200 mm or above </strong>. While there are some overstrain failures at low toolwear values, however, OSF is majorly a result of higher toolwear as can be observed through the violin plots. This observation is fairly consistent for each of the three machine types.   üî©.\n</div>\n\n\n# Feature Engineering\n\nWe will majorly concentrate on two new features. These features have already been analysed in sections \\@ref(label1) and \\@ref(label2). These __features are namely temperature difference and power consumption.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train <- df_train %>% mutate(temp_diff_k = process_temperature_k - air_temperature_k) %>% select(-c(process_temperature_k,air_temperature_k))\ndf_train <- df_train %>% mutate(power_w = torque_nm * 2* pi * rotational_speed_rpm/60) %>% select(-c(torque_nm,rotational_speed_rpm))\n```\n:::\n\n\n\n\nIn the next step, we will encode the character variable for machine type into machine readable format __by one hot encoding the variable__ as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train$type <- factor(df_train$type)\ndt_train <- data.table(df_train)\ndt_train <- one_hot(dt_train,cols = as.factor(\"type\"))\n\ndf_train <- as.data.frame(dt_train)\n```\n:::\n\n\n# Correlation plot\n\nAfter analysing the various variables and performing feature engineering, let us create a correlation plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot(cor(df_train),        # Correlation matrix\n         method = \"number\", # Correlation plot method\n         type = \"full\",    # Correlation plot style (also \"upper\" and \"lower\")\n         diag = TRUE,      # If TRUE (default), adds the diagonal\n         tl.col = \"black\", # Labels color\n         bg = \"white\",     # Background color\n         title = \"\",       # Main title\n         col = NULL,\n         number.cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](Machine_failure_preds_files/figure-html/corrplot-1.png){width=672}\n:::\n:::\n\n\n\n\n<div class=\"alert alert-info\">\n üí° Based on the above correlation plot, we can observe that machine failure has <strong> high correlation with TWF,PSF,HDF,OSF and RNF </strong>. This indicates that a failure for any of these individual variables could lead to machine failure üí°.\n</div>\n\n# Classification model\n\nIn order to create a classification, we must first segregate the dataset into train and test datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\nsample=sample.split(df_train$machine_failure,SplitRatio=0.7)\ntrain=subset(df_train,sample==T)\ntest=subset(df_train,sample==F)\n```\n:::\n\n\nAfter creating the required train and test dataframes, we now train the dataset by applying various classification algorithms. These have been delineated in the following sections.\n\n## Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_logit <- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfitting null model for pseudo-r2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n```\n:::\n:::\n\n<div class=\"alert alert-info\">\n üí° Upon studying the McFadden $R^2$ value, we observe that the <strong>model accuracy was approximately 71.2% </strong> üí°.\n</div>\n\n\nLet us now observe how well we can predict on the test dataset based on the logistic regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.results <- predict(model_logit,newdata=subset(test,select=-(machine_failure)),type='response')\nfitted.results <- ifelse(fitted.results > 0.5,1,0)\n\nmisClasificError <- mean(fitted.results != test$machine_failure)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n```\n:::\n:::\n\n\n\nAs we can observe, the logistic regression model was able to __accurately predict 97.2% of the machine failures__.\n\nLet us further study the performance of the logistic regression model through the Receiver Operating Curve (ROC) metric.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- as.numeric(predict(model_logit, newdata=subset(test,select=-c(machine_failure)), type=\"response\"))\npr <- prediction(p, test$machine_failure)\nprf <- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\nplot(prf)\nauc_logit <- performance(pr, measure = \"auc\")\nauc_logit <- auc_logit@y.values[[1]]\ntitle(\"Receiver Operating Curve for Logistic Regression\")\n```\n\n::: {.cell-output-display}\n![Receiver Operating Curve for Logistic Regression](Machine_failure_preds_files/figure-html/roc-logit-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nBased on the ROC as illustrated by figure \\@ref(fig:roc-logit), we can observe that a large section of the upper half of the plot has been covered by the operating curve. \n\n\n\n<div class=\"alert alert-info\">\n üí° The Area Under Curve (AUC) score of 0.9261326 suggests that the model was able to <strong> predict the machine failures fairly well. </strong> üí°\n</div>\n\n\nNext, we try to obtain a more intuitive performance metric of the model by creating a confusion matrix.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_confusion_matrix <- function(cm) {\n\n  layout(matrix(c(1,1,2)))\n  par(mar=c(2,2,2,2))\n  plot(c(100, 345), c(300, 450), type = \"n\", xlab=\"\", ylab=\"\", xaxt='n', yaxt='n')\n  title('CONFUSION MATRIX', cex.main=2)\n\n  # create the matrix \n  rect(150, 430, 240, 370, col='#3F97D0')\n  text(195, 435, 'False', cex=1.2)\n  rect(250, 430, 340, 370, col='#F7AD50')\n  text(295, 435, 'True', cex=1.2)\n  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)\n  text(245, 450, 'Actual', cex=1.3, font=2)\n  rect(150, 305, 240, 365, col='#F7AD50')\n  rect(250, 305, 340, 365, col='#3F97D0')\n  text(140, 400, 'False', cex=1.2, srt=90)\n  text(140, 335, 'True', cex=1.2, srt=90)\n\n  # add in the cm results \n  res <- as.numeric(cm$table)\n  text(195, 400, res[1], cex=1.6, font=2, col='white')\n  text(195, 335, res[2], cex=1.6, font=2, col='white')\n  text(295, 400, res[3], cex=1.6, font=2, col='white')\n  text(295, 335, res[4], cex=1.6, font=2, col='white')\n\n  # add in the specifics \n  plot(c(100, 0), c(100, 0), type = \"n\", xlab=\"\", ylab=\"\", main = \"DETAILS\", xaxt='n', yaxt='n')\n  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)\n  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)\n  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)\n  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)\n  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)\n  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)\n  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)\n  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)\n  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)\n  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)\n\n  # add in the accuracy information \n  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)\n  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)\n  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)\n  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)\n}  \n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_logit <- confusionMatrix(factor(fitted.results),factor(test$machine_failure))\ndraw_confusion_matrix(cm_logit)\n```\n\n::: {.cell-output-display}\n![Confusion matrix for Logistic Regression](Machine_failure_preds_files/figure-html/conf-mat-1.png){fig-align='center' width=672}\n:::\n:::\n\nFigure \\@ref(fig:conf-mat) illustrates the confusion matrix for the logistic regression model along with its various performance metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_id <- df_test$id\ndf_test <- df_test %>% select(-c(id,`Product ID`))\ndf_test <- clean_names(df_test)\ndf_test$type <- factor(df_test$type)\ndf_test<- data.table(df_test)\ndf_test <- one_hot(df_test,cols = as.factor(\"type\"))\n\ndf_test <- as.data.frame(df_test)\n\ndf_test <- df_test %>% mutate(temp_diff_k = process_temperature_k - air_temperature_k) %>% select(-c(process_temperature_k,air_temperature_k))\ndf_test <- df_test %>% mutate(power_w = torque_nm * 2* pi * rotational_speed_rpm/60) %>% select(-c(torque_nm,rotational_speed_rpm))\n\ndf_test <- df_test %>% select(c(\"type_H\",\"type_L\",\"type_M\",\"tool_wear_min\",\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\",\"power_w\",\"temp_diff_k\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.results <- predict(model_logit,df_test,type='response')\nfitted.results <- as.data.frame(ifelse(fitted.results > 0.5,1,0))\nfitted.results <- fitted.results %>% rename(\"Machine failure\" = \"ifelse(fitted.results > 0.5, 1, 0)\")\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Logistic_Reg_predictions.csv\")\n```\n:::\n\n\n## Random Forest\n\nLet us use an ensemble algorithm to classify our results. We shall utilise the Random Forest technique which utilises multiple decision trees to predict results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model<-randomForest(machine_failure~.,data=train)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(rf_model)\n```\n\n::: {.cell-output-display}\n![Error vs Number of trees for Random Forest](Machine_failure_preds_files/figure-html/errormod-1.png){fig-align='center' width=672}\n:::\n:::\n\nAs we can observe from figure \\@ref(fig:errormod), \n\n\n<div class=\"alert alert-info\">\n üí° the error of the random forest model is observed to reduce <strong> as the number of trees cross 100 </strong>. </strong> üí°\n</div>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_rf <- predict(rf_model,test, type = 'class')\npred_rf <- if_else(pred_rf > 0.3,1,0)\ncm_rf <- confusionMatrix(factor(pred_rf),factor(test$machine_failure))\ndraw_confusion_matrix(cm_rf)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the random forest model](Machine_failure_preds_files/figure-html/conf-mat-rf-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.results <- predict(rf_model,df_test,type='class')\nfitted.results <- as.data.frame(ifelse(fitted.results > 0.3,1,0))\nfitted.results <- fitted.results %>% rename(\"Machine failure\" = \"ifelse(fitted.results > 0.3, 1, 0)\")\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"RF_predictions.csv\")\n```\n:::\n\n\n## XGboost\n\nLet us try to use an extra gradient boosted ensemble method commonly termed as the XGboost classifier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_model <- xgboost(data = as.matrix(train %>% select(-c(machine_failure))), label = as.matrix(train$machine_failure), \n                     max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = \"binary:logistic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-logloss:0.144407 \n[2]\ttrain-logloss:0.059787 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_xgb <- predict(xgb_model, as.matrix(test %>% select(-c(machine_failure))))\npred_xgb <- if_else(pred_xgb > 0.3,1,0)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncm_xgb <- confusionMatrix(factor(pred_xgb),factor(test$machine_failure))\ndraw_confusion_matrix(cm_xgb)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the XGBoost model](Machine_failure_preds_files/figure-html/conf-mat-xgb-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict(xgb_model,as.matrix(df_test))\nfitted.results <- as.data.frame(preds)\nfitted.results <- fitted.results %>% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\n\nwrite_csv(fitted.results,\"XGB_predictions.csv\")\n```\n:::\n\n\n\n## Light GBM\n\nLet us utilise the LGBM algorithm and train it on the given dataset.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_lgb = predict(model_lgb, as.matrix(test))\npred_lgb <- if_else(pred_lgb > 0.3,1,0)\ncm_lgb <- confusionMatrix(factor(pred_lgb),factor(test$machine_failure))\ndraw_confusion_matrix(cm_lgb)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Light GB model](Machine_failure_preds_files/figure-html/cf-lgb-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict(model_lgb,as.matrix(df_test),predict_disable_shape_check=TRUE)\nfitted.results <- as.data.frame(preds)\nfitted.results <- fitted.results %>% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"LGBM_predictions_std.csv\")\n```\n:::\n\n\n\n## Catboost\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimportance <- varImp(model_cat, scale = FALSE)\nplot(importance)\n```\n\n::: {.cell-output-display}\n![Feature importance using Catboost model](Machine_failure_preds_files/figure-html/catb-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_cat = predict(model_cat, test %>% select(-machine_failure))\npred_cat <- if_else(pred_cat == \"X0\",0,1)\ncm_cat <- confusionMatrix(factor(pred_cat),factor(test$machine_failure))\ndraw_confusion_matrix(cm_cat)\n```\n\n::: {.cell-output-display}\n![Confusion matrix of the Catboost model](Machine_failure_preds_files/figure-html/cf-cat-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.results <- predict(model_cat,df_test)\nfitted.results <- as.data.frame(ifelse(fitted.results == \"X0\",0,1))\nfitted.results <- fitted.results %>% rename_with(.cols = 1, ~\"Machine failure\")\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Cat_predictions.csv\")\n```\n:::\n\n\n\n\n## Feature transformation\n\nNow that we have created all our baseline models, let us try our hand out with some feature transformation with standard scaling options.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain$tool_wear_min <- train$tool_wear_min %>% scale(center=TRUE,scale=TRUE)\ntrain$power_w <- train$power_w %>% scale(center=TRUE,scale=TRUE)\ntrain$temp_diff_k <- train$temp_diff_k %>% scale(center=TRUE,scale=TRUE)\n\ntest$tool_wear_min <- test$tool_wear_min %>% scale(center=TRUE,scale=TRUE)\ntest$power_w <- test$power_w %>% scale(center=TRUE,scale=TRUE)\ntest$temp_diff_k <- test$temp_diff_k %>% scale(center=TRUE,scale=TRUE)\n\n\ncols <- c(\"twf\",\"hdf\",\"pwf\",\"osf\",\"rnf\")\n\n\ntrain %<>%\n       mutate_each_(funs(factor(.)),cols)   #Converting  to factors\ntest %<>%\n       mutate_each_(funs(factor(.)),cols)\ndf_test %<>%\n       mutate_each_(funs(factor(.)),cols)\n```\n:::\n\n\nNow that we have standardised all the continuous numeric variables, let us attempt to train the model once again on the scaled dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_logit <- glm(machine_failure~.,family=binomial(link='logit'),data=train)\npR2(model_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfitting null model for pseudo-r2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n          llh       llhNull            G2      McFadden          r2ML \n-2224.7236061 -7735.2112184 11020.9752246     0.7123901     0.1089918 \n         r2CU \n    0.7287878 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted.results <- predict(model_logit,newdata=subset(test,select=-(machine_failure)),type='response')\nfitted.results <- ifelse(fitted.results > 0.5,1,0)\n\nmisClasificError <- mean(fitted.results != test$machine_failure)\nprint(paste('Accuracy of logistic regression:',1-misClasificError))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy of logistic regression: 0.996090695856138\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_test$tool_wear_min <- df_test$tool_wear_min %>% scale(center=TRUE,scale=TRUE)\ndf_test$power_w <- df_test$power_w %>% scale(center=TRUE,scale=TRUE)\ndf_test$temp_diff_k <- df_test$temp_diff_k %>% scale(center=TRUE,scale=TRUE)\n\n\npreds <- predict(model_logit,df_test,type='response')\nfitted.results <- as.data.frame(preds)\nfitted.results <- fitted.results %>% rename(\"Machine failure\" = \"preds\" )\nfitted.results$id <- test_id\nfitted.results <- fitted.results %>% select(c(\"id\",\"Machine failure\"))\n\nwrite_csv(fitted.results,\"Logistic_Reg_predictions_std.csv\")\n```\n:::\n",
    "supporting": [
      "Machine_failure_preds_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}